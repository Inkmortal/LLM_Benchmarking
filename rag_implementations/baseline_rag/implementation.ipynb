{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline RAG Implementation\n",
    "\n",
    "This notebook implements a generic baseline RAG system that can be used with any source material. It uses:\n",
    "- Cohere Embed English (cohere.embed-english-v3) for embeddings\n",
    "- Claude 3.5 Sonnet for LLM responses\n",
    "- Amazon OpenSearch for vector storage\n",
    "\n",
    "## Features\n",
    "- Generic document ingestion\n",
    "- Vector similarity search\n",
    "- Context-aware response generation\n",
    "- Automatic retry with exponential backoff\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Vector Search\n",
    "- k: Number of context documents to retrieve (default: 3)\n",
    "- search_type: Type of vector search to use ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score to include (default: None)\n",
    "\n",
    "### OpenSearch\n",
    "- index_settings: Custom index settings for performance tuning\n",
    "- knn_params: Parameters for k-NN algorithm (e.g., ef_search)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum number of retry attempts (default: 5)\n",
    "- min_delay: Minimum delay between retries in seconds (default: 1)\n",
    "- max_delay: Maximum delay between retries in seconds (default: 60)\n",
    "\n",
    "## Usage\n",
    "1. Initialize the RAG system with desired configuration\n",
    "2. Ingest documents (text content with optional metadata)\n",
    "3. Query the system with natural language questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, helpers\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from utils.notebook_utils.importable import notebook_to_module\n",
    "\n",
    "# Import ingestion functionality\n",
    "ingestion = notebook_to_module(str(Path(__file__).parent / 'ingestion.ipynb'))\n",
    "ingest_documents = ingestion.ingest_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRAG:\n",
    "    \"\"\"Generic baseline RAG implementation with configurable parameters\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        index_name: str = \"rag-documents\",\n",
    "        # Document processing config\n",
    "        chunk_size: int = 500,  # Default 500 words â‰ˆ 2000 chars\n",
    "        chunk_overlap: int = 50,  # Default 50 words overlap\n",
    "        enable_chunking: bool = True,\n",
    "        # Vector search config\n",
    "        search_type: Literal['script', 'knn'] = 'script',\n",
    "        similarity_threshold: Optional[float] = None,\n",
    "        # OpenSearch config\n",
    "        index_settings: Optional[Dict] = None,\n",
    "        knn_params: Optional[Dict] = None,\n",
    "        # API config\n",
    "        max_retries: int = 5,\n",
    "        min_delay: float = 1.0,\n",
    "        max_delay: float = 60.0\n",
    "    ):\n",
    "        # Initialize AWS services\n",
    "        self.bedrock = boto3.client('bedrock-runtime')\n",
    "        self.region = boto3.Session().region_name\n",
    "        \n",
    "        # OpenSearch configuration\n",
    "        self.opensearch_host = os.getenv('OPENSEARCH_HOST')\n",
    "        if not self.opensearch_host:\n",
    "            raise ValueError(\"OPENSEARCH_HOST environment variable is required\")\n",
    "        \n",
    "        credentials = boto3.Session().get_credentials()\n",
    "        self.awsauth = AWS4Auth(\n",
    "            credentials.access_key,\n",
    "            credentials.secret_key,\n",
    "            self.region,\n",
    "            'es',\n",
    "            session_token=credentials.token\n",
    "        )\n",
    "        \n",
    "        self.opensearch = OpenSearch(\n",
    "            hosts=[{'host': self.opensearch_host, 'port': 443}],\n",
    "            http_auth=self.awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )\n",
    "        \n",
    "        # Model configuration\n",
    "        self.embedding_model_id = \"cohere.embed-english-v3\"\n",
    "        self.llm_model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Document processing configuration\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.enable_chunking = enable_chunking\n",
    "        \n",
    "        # Vector search configuration\n",
    "        self.search_type = search_type\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        \n",
    "        # OpenSearch configuration\n",
    "        self.index_settings = index_settings or {}\n",
    "        self.knn_params = knn_params or {}\n",
    "        \n",
    "        # API configuration\n",
    "        self.max_retries = max_retries\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        \n",
    "        # Ensure index exists\n",
    "        self._create_index_if_not_exists()\n",
    "    \n",
    "    def _create_index_if_not_exists(self):\n",
    "        \"\"\"Create OpenSearch index with appropriate mapping and settings\"\"\"\n",
    "        if not self.opensearch.indices.exists(self.index_name):\n",
    "            # Default settings\n",
    "            settings = {\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0,\n",
    "                \"knn\": {\n",
    "                    \"algo_param\": {\n",
    "                        \"ef_search\": 512  # Higher values = more accurate but slower\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Update with custom settings\n",
    "            settings.update(self.index_settings)\n",
    "            \n",
    "            # Default mapping\n",
    "            mapping = {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\"},\n",
    "                    \"metadata\": {\"type\": \"object\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": 1024,  # Cohere embedding dimension\n",
    "                        \"method\": {\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"parameters\": {\n",
    "                                \"ef_construction\": 512,\n",
    "                                \"m\": 16\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Update KNN parameters\n",
    "            if self.knn_params:\n",
    "                mapping[\"properties\"][\"embedding\"][\"method\"][\"parameters\"].update(self.knn_params)\n",
    "            \n",
    "            self.opensearch.indices.create(\n",
    "                index=self.index_name,\n",
    "                body={\n",
    "                    \"settings\": settings,\n",
    "                    \"mappings\": mapping\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    def _invoke_with_retry(self, model_id: str, body: Dict) -> Dict:\n",
    "        \"\"\"Invoke Bedrock model with exponential backoff retry\n",
    "        \n",
    "        Args:\n",
    "            model_id: Bedrock model ID\n",
    "            body: Request body\n",
    "            \n",
    "        Returns:\n",
    "            Model response\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If max retries exceeded\n",
    "        \"\"\"\n",
    "        last_exception = None\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.bedrock.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    body=json.dumps(body)\n",
    "                )\n",
    "                return json.loads(response['body'].read())\n",
    "                \n",
    "            except ClientError as e:\n",
    "                last_exception = e\n",
    "                if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        raise\n",
    "                    # Exponential backoff with jitter\n",
    "                    delay = min(\n",
    "                        self.max_delay,\n",
    "                        self.min_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    raise\n",
    "                    \n",
    "        raise last_exception\n",
    "    \n",
    "    def get_embeddings(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embeddings using Cohere model\"\"\"\n",
    "        request_body = {\n",
    "            \"texts\": [text],\n",
    "            \"input_type\": \"search_document\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response_body = self._invoke_with_retry(\n",
    "                model_id=self.embedding_model_id,\n",
    "                body=request_body\n",
    "            )\n",
    "            return response_body['embeddings'][0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings: {str(e)}\")\n",
    "            print(f\"Text length: {len(text)} chars, {len(text.split())} words\")\n",
    "            raise\n",
    "    \n",
    "    def _store_documents(self, documents: List[Dict[str, Any]], batch_size: int = 100) -> None:\n",
    "        \"\"\"Store documents in OpenSearch with embeddings.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of dictionaries with 'content' and optional 'metadata'\n",
    "            batch_size: Number of documents to process in each batch\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "            if 'content' not in doc:\n",
    "                raise ValueError(\"Each document must have 'content' field\")\n",
    "                \n",
    "            # Generate embedding\n",
    "            embedding = self.get_embeddings(doc['content'])\n",
    "            \n",
    "            # Prepare document for indexing\n",
    "            action = {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"content\": doc['content'],\n",
    "                    \"metadata\": doc.get('metadata', {}),\n",
    "                    \"embedding\": embedding\n",
    "                }\n",
    "            }\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Bulk index when batch is full\n",
    "            if len(actions) >= batch_size:\n",
    "                helpers.bulk(self.opensearch, actions)\n",
    "                actions = []\n",
    "        \n",
    "        # Index any remaining documents\n",
    "        if actions:\n",
    "            helpers.bulk(self.opensearch, actions)\n",
    "    \n",
    "    def ingest_documents(self, documents: List[Dict[str, Any]], batch_size: int = 100) -> None:\n",
    "        \"\"\"Ingest documents into vector store with optional chunking.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of dictionaries with 'content' and optional 'metadata'\n",
    "            batch_size: Number of documents to process in each batch\n",
    "        \"\"\"\n",
    "        # Use ingestion notebook's functionality with our configuration\n",
    "        ingest_documents(\n",
    "            documents, \n",
    "            self, \n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            enable_chunking=self.enable_chunking,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    \n",
    "    def semantic_search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for relevant documents using embeddings\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of documents with content and metadata\n",
    "        \"\"\"\n",
    "        query_embedding = self.get_embeddings(query)\n",
    "        \n",
    "        if self.search_type == 'script':\n",
    "            # Script-based cosine similarity search\n",
    "            script_query = {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"lang\": \"painless\",\n",
    "                        \"source\": \"double score = cosineSimilarity(params.query_vector, doc['embedding']); return score + 1.0;\",\n",
    "                        \"params\": {\"query_vector\": query_embedding}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add minimum score if threshold is set\n",
    "            if self.similarity_threshold is not None:\n",
    "                script_query[\"script_score\"][\"min_score\"] = self.similarity_threshold\n",
    "            \n",
    "            response = self.opensearch.search(\n",
    "                index=self.index_name,\n",
    "                body={\n",
    "                    \"size\": k,\n",
    "                    \"query\": script_query,\n",
    "                    \"_source\": [\"content\", \"metadata\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        else:  # search_type == 'knn'\n",
    "            # Pure k-NN search\n",
    "            knn_query = {\n",
    "                \"knn\": {\n",
    "                    \"embedding\": {\n",
    "                        \"vector\": query_embedding,\n",
    "                        \"k\": k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.opensearch.search(\n",
    "                index=self.index_name,\n",
    "                body={\n",
    "                    \"query\": knn_query,\n",
    "                    \"_source\": [\"content\", \"metadata\"]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        return [hit['_source'] for hit in response['hits']['hits']]\n",
    "    \n",
    "    def generate_response(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Generate response using Claude 3.5 Sonnet\"\"\"\n",
    "        context_str = \"\\n\\n\".join([doc['content'] for doc in context])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question. \n",
    "        If you cannot answer the question based on the context, say so.\n",
    "        \n",
    "        Context:\n",
    "        {context_str}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response_body = self._invoke_with_retry(\n",
    "            model_id=self.llm_model_id,\n",
    "            body=request_body\n",
    "        )\n",
    "        \n",
    "        return response_body['content'][0]['text']\n",
    "    \n",
    "    def query(self, query: str, k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Complete RAG pipeline\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language question\n",
    "            k: Number of context documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - query: Original question\n",
    "            - context: Retrieved relevant documents\n",
    "            - response: Generated answer\n",
    "        \"\"\"\n",
    "        # Get relevant documents\n",
    "        context = self.semantic_search(query, k)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query, context)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"context\": context,\n",
    "            \"response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with different configurations\n",
    "def test_rag_system():\n",
    "    # Example 1: Default configuration\n",
    "    print(\"Testing default configuration...\")\n",
    "    rag = BaselineRAG(index_name=\"test-rag-default\")\n",
    "    \n",
    "    # Example 2: Custom chunking\n",
    "    print(\"\\nTesting custom chunking...\")\n",
    "    rag_custom_chunks = BaselineRAG(\n",
    "        index_name=\"test-rag-custom-chunks\",\n",
    "        chunk_size=300,  # Smaller chunks\n",
    "        chunk_overlap=100,  # More overlap\n",
    "        enable_chunking=True\n",
    "    )\n",
    "    \n",
    "    # Example 3: k-NN search with custom parameters\n",
    "    print(\"\\nTesting k-NN search...\")\n",
    "    rag_knn = BaselineRAG(\n",
    "        index_name=\"test-rag-knn\",\n",
    "        search_type='knn',\n",
    "        knn_params={\n",
    "            \"ef_construction\": 1024,  # Higher = more accurate index\n",
    "            \"m\": 32  # Higher = more connections per node\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        {\n",
    "            \"content\": \"Machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from data.\",\n",
    "            \"metadata\": {\"source\": \"test\", \"topic\": \"ML\"}\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Deep learning is a type of machine learning that uses neural networks with multiple layers.\",\n",
    "            \"metadata\": {\"source\": \"test\", \"topic\": \"DL\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test each configuration\n",
    "    for name, rag_instance in [\n",
    "        (\"Default\", rag),\n",
    "        (\"Custom Chunks\", rag_custom_chunks),\n",
    "        (\"k-NN Search\", rag_knn)\n",
    "    ]:\n",
    "        print(f\"\\nTesting {name} Configuration:\")\n",
    "        print(\"Ingesting documents...\")\n",
    "        rag_instance.ingest_documents(documents)\n",
    "        \n",
    "        print(\"Testing query...\")\n",
    "        result = rag_instance.query(\"What is machine learning?\")\n",
    "        \n",
    "        print(\"Response:\", result['response'])\n",
    "        print(\"\\nContext used:\")\n",
    "        for doc in result['context']:\n",
    "            print(f\"- {doc['content']}\")\n",
    "            print(f\"  Metadata: {doc['metadata']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_rag_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
