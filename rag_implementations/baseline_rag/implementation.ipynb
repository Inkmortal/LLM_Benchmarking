{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline RAG Implementation\n",
    "\n",
    "This notebook implements a generic baseline RAG system that can be used with any source material. It uses:\n",
    "\n",
    "- Cohere Embed English (cohere.embed-english-v3) for embeddings\n",
    "- Claude 3.5 Sonnet for LLM responses\n",
    "- Amazon OpenSearch for vector storage\n",
    "\n",
    "## Features\n",
    "- Generic document ingestion\n",
    "- Vector similarity search\n",
    "- Context-aware response generation\n",
    "- Automatic retry with exponential backoff\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Vector Search\n",
    "- k: Number of context documents to retrieve (default: 3)\n",
    "- search_type: Type of vector search to use ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score to include (default: None)\n",
    "\n",
    "### OpenSearch\n",
    "- index_settings: Custom index settings for performance tuning\n",
    "- knn_params: Parameters for k-NN algorithm (e.g., ef_search)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum number of retry attempts (default: 5)\n",
    "- min_delay: Minimum delay between retries in seconds (default: 1)\n",
    "- max_delay: Maximum delay between retries in seconds (default: 60)\n",
    "\n",
    "## Usage\n",
    "1. Initialize the RAG system with desired configuration\n",
    "2. Ingest documents (text content with optional metadata)\n",
    "3. Query the system with natural language questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from utils.aws.opensearch import (\n",
    "    OpenSearchConfig,\n",
    "    VectorSearchConfig,\n",
    "    OpenSearchManager,\n",
    "    VectorStore\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module\n",
    "\n",
    "# Import ingestion functionality using relative path\n",
    "ingestion = notebook_to_module('ingestion.ipynb')\n",
    "ingest_documents = ingestion.ingest_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRAG:\n",
    "    \"\"\"Generic baseline RAG implementation with configurable parameters\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        index_name: str = \"rag-documents\",\n",
    "        # Document processing config\n",
    "        chunk_size: int = 500,  # Default 500 words â‰ˆ 2000 chars\n",
    "        chunk_overlap: int = 50,  # Default 50 words overlap\n",
    "        enable_chunking: bool = True,\n",
    "        # Vector search config\n",
    "        search_type: Literal['script', 'knn'] = 'script',\n",
    "        similarity_threshold: Optional[float] = None,\n",
    "        # OpenSearch config\n",
    "        index_settings: Optional[Dict] = None,\n",
    "        knn_params: Optional[Dict] = None,\n",
    "        # API config\n",
    "        max_retries: int = 5,\n",
    "        min_delay: float = 1.0,\n",
    "        max_delay: float = 60.0\n",
    "    ):\n",
    "        # Initialize AWS services\n",
    "        self.bedrock = boto3.client('bedrock-runtime')\n",
    "        self.region = boto3.Session().region_name\n",
    "        \n",
    "        # Model configuration\n",
    "        self.embedding_model_id = \"cohere.embed-english-v3\"\n",
    "        self.llm_model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Document processing configuration\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.enable_chunking = enable_chunking\n",
    "        \n",
    "        # API configuration\n",
    "        self.max_retries = max_retries\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        \n",
    "        # Initialize OpenSearch components\n",
    "        # Use shorter domain name to stay under 28 char limit\n",
    "        domain_suffix = index_name.split('-')[0][:10]  # Take first 10 chars of first segment\n",
    "        opensearch_config = OpenSearchConfig(\n",
    "            domain_name=f\"blrag-{domain_suffix}\",  # Short prefix + truncated name\n",
    "            cleanup_enabled=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        vector_config = VectorSearchConfig(\n",
    "            search_type=search_type,\n",
    "            similarity_threshold=similarity_threshold,\n",
    "            index_settings=index_settings,\n",
    "            knn_params=knn_params,\n",
    "            max_retries=max_retries,\n",
    "            min_delay=min_delay,\n",
    "            max_delay=max_delay\n",
    "        )\n",
    "        \n",
    "        # Set up OpenSearch domain\n",
    "        self.opensearch_manager = OpenSearchManager(opensearch_config)\n",
    "        endpoint = self.opensearch_manager.setup_domain()\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.vector_store = VectorStore(\n",
    "            index_name=index_name,\n",
    "            config=vector_config,\n",
    "            client=self.opensearch_manager.client\n",
    "        )\n",
    "    \n",
    "    def _invoke_with_retry(self, model_id: str, body: Dict) -> Dict:\n",
    "        \"\"\"Invoke Bedrock model with exponential backoff retry\n",
    "        \n",
    "        Args:\n",
    "            model_id: Bedrock model ID\n",
    "            body: Request body\n",
    "            \n",
    "        Returns:\n",
    "            Model response\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If max retries exceeded\n",
    "        \"\"\"\n",
    "        last_exception = None\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.bedrock.invoke_model(\n",
    "                    modelId=model_id,\n",
    "                    body=json.dumps(body)\n",
    "                )\n",
    "                return json.loads(response['body'].read())\n",
    "                \n",
    "            except ClientError as e:\n",
    "                last_exception = e\n",
    "                if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        raise\n",
    "                    # Exponential backoff with jitter\n",
    "                    delay = min(\n",
    "                        self.max_delay,\n",
    "                        self.min_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    raise\n",
    "                    \n",
    "        raise last_exception\n",
    "    \n",
    "    def get_embeddings(self, text: str) -> Optional[List[float]]:\n",
    "        \"\"\"Generate embeddings using Cohere model\"\"\"\n",
    "        request_body = {\n",
    "            \"texts\": [text],\n",
    "            \"input_type\": \"search_document\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response_body = self._invoke_with_retry(\n",
    "                model_id=self.embedding_model_id,\n",
    "                body=request_body\n",
    "            )\n",
    "            return response_body['embeddings'][0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings: {str(e)}\")\n",
    "            print(f\"Text length: {len(text)} chars, {len(text.split())} words\")\n",
    "            return None\n",
    "    \n",
    "    def _store_documents(self, documents: List[Dict[str, Any]], batch_size: int = 100) -> None:\n",
    "        \"\"\"Store documents in OpenSearch with embeddings.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of dictionaries with 'content' and optional 'metadata'\n",
    "            batch_size: Number of documents to process in each batch\n",
    "        \"\"\"\n",
    "        # Process documents to add embeddings\n",
    "        docs_with_vectors = []\n",
    "        for doc in documents:\n",
    "            if 'content' not in doc:\n",
    "                print(\"Document missing 'content' field\")\n",
    "                continue\n",
    "                \n",
    "            # Generate embedding\n",
    "            vector = self.get_embeddings(doc['content'])\n",
    "            if vector is None:\n",
    "                print(f\"Failed to get embeddings for document {doc.get('id', 'unknown')}\")\n",
    "                continue\n",
    "            \n",
    "            docs_with_vectors.append({\n",
    "                'content': doc['content'],\n",
    "                'vector': vector,\n",
    "                'metadata': doc.get('metadata', {})\n",
    "            })\n",
    "        \n",
    "        if not docs_with_vectors:\n",
    "            print(\"No valid documents to store\")\n",
    "            return\n",
    "        \n",
    "        # Store documents in batches\n",
    "        self.vector_store.store_documents(docs_with_vectors, batch_size=batch_size)\n",
    "    \n",
    "    def semantic_search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for relevant documents using embeddings\n",
    "        \n",
    "        Args:\n",
    "            query: Search query text\n",
    "            k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of documents with content and metadata\n",
    "        \"\"\"\n",
    "        query_vector = self.get_embeddings(query)\n",
    "        if query_vector is None:\n",
    "            print(\"Failed to get embeddings for query\")\n",
    "            return []\n",
    "            \n",
    "        return self.vector_store.search(query_vector, k=k)\n",
    "    \n",
    "    def generate_response(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Generate response using Claude 3.5 Sonnet\"\"\"\n",
    "        context_str = \"\\n\\n\".join([doc['content'] for doc in context])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question. \n",
    "        If you cannot answer the question based on the context, say so.\n",
    "        \n",
    "        Context:\n",
    "        {context_str}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response_body = self._invoke_with_retry(\n",
    "            model_id=self.llm_model_id,\n",
    "            body=request_body\n",
    "        )\n",
    "        \n",
    "        return response_body['content'][0]['text']\n",
    "    \n",
    "    def query(self, query: str, k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Complete RAG pipeline\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language question\n",
    "            k: Number of context documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - query: Original question\n",
    "            - context: Retrieved relevant documents\n",
    "            - response: Generated answer\n",
    "        \"\"\"\n",
    "        # Get relevant documents\n",
    "        context = self.semantic_search(query, k)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query, context)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"context\": context,\n",
    "            \"response\": response\n",
    "        }\n",
    "    \n",
    "    def cleanup(self, delete_resources: bool = False):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        try:\n",
    "            self.vector_store.cleanup(delete_resources=delete_resources)\n",
    "            if delete_resources:\n",
    "                self.opensearch_manager.cleanup()\n",
    "        except:\n",
    "            pass  # Best effort cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
