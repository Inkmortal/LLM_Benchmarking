{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline RAG Implementation\n",
    "\n",
    "This notebook implements a generic baseline RAG system that can be used with any source material. It uses:\n",
    "- Cohere Embed English (cohere.embed-english-v3) for embeddings\n",
    "- Claude 3.5 Sonnet for LLM responses\n",
    "- Amazon OpenSearch for vector storage\n",
    "\n",
    "## Features\n",
    "- Generic document ingestion\n",
    "- Vector similarity search\n",
    "- Context-aware response generation\n",
    "\n",
    "## Usage\n",
    "1. Initialize the RAG system\n",
    "2. Ingest documents (text content with optional metadata)\n",
    "3. Query the system with natural language questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from typing import List, Dict, Any, Optional\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, helpers\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AWSConfig:\n",
    "    \"\"\"AWS service configuration\"\"\"\n",
    "    def __init__(self):\n",
    "        self.bedrock = boto3.client('bedrock-runtime')\n",
    "        self.region = boto3.Session().region_name\n",
    "        \n",
    "        # OpenSearch configuration\n",
    "        self.opensearch_host = os.getenv('OPENSEARCH_HOST')\n",
    "        if not self.opensearch_host:\n",
    "            raise ValueError(\"OPENSEARCH_HOST environment variable is required\")\n",
    "        \n",
    "        credentials = boto3.Session().get_credentials()\n",
    "        self.awsauth = AWS4Auth(\n",
    "            credentials.access_key,\n",
    "            credentials.secret_key,\n",
    "            self.region,\n",
    "            'es',\n",
    "            session_token=credentials.token\n",
    "        )\n",
    "        \n",
    "        self.opensearch = OpenSearch(\n",
    "            hosts=[{'host': self.opensearch_host, 'port': 443}],\n",
    "            http_auth=self.awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BaselineRAG:\n",
    "    \"\"\"Generic baseline RAG implementation\"\"\"\n",
    "    def __init__(self, config: AWSConfig, index_name: str = \"rag-documents\"):\n",
    "        self.config = config\n",
    "        self.embedding_model_id = \"cohere.embed-english-v3\"\n",
    "        self.llm_model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Ensure index exists\n",
    "        self._create_index_if_not_exists()\n",
    "    \n",
    "    def _create_index_if_not_exists(self):\n",
    "        \"\"\"Create OpenSearch index with appropriate mapping\"\"\"\n",
    "        if not self.config.opensearch.indices.exists(self.index_name):\n",
    "            mapping = {\n",
    "                \"mappings\": {\n",
    "                    \"properties\": {\n",
    "                        \"content\": {\"type\": \"text\"},\n",
    "                        \"metadata\": {\"type\": \"object\"},\n",
    "                        \"embedding\": {\n",
    "                            \"type\": \"knn_vector\",\n",
    "                            \"dimension\": 1024  # Cohere embedding dimension\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            self.config.opensearch.indices.create(\n",
    "                index=self.index_name,\n",
    "                body=mapping\n",
    "            )\n",
    "    \n",
    "    def get_embeddings(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embeddings using Cohere model\"\"\"\n",
    "        request_body = {\n",
    "            \"inputText\": text\n",
    "        }\n",
    "        \n",
    "        response = self.config.bedrock.invoke_model(\n",
    "            modelId=self.embedding_model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['embeddings']\n",
    "    \n",
    "    def ingest_documents(self, documents: List[Dict[str, Any]], batch_size: int = 100) -> None:\n",
    "        \"\"\"Ingest documents into vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of dictionaries with 'content' and optional 'metadata'\n",
    "            batch_size: Number of documents to process in each batch\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "            if 'content' not in doc:\n",
    "                raise ValueError(\"Each document must have 'content' field\")\n",
    "                \n",
    "            # Generate embedding\n",
    "            embedding = self.get_embeddings(doc['content'])\n",
    "            \n",
    "            # Prepare document for indexing\n",
    "            action = {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"content\": doc['content'],\n",
    "                    \"metadata\": doc.get('metadata', {}),\n",
    "                    \"embedding\": embedding\n",
    "                }\n",
    "            }\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Bulk index when batch is full\n",
    "            if len(actions) >= batch_size:\n",
    "                helpers.bulk(self.config.opensearch, actions)\n",
    "                actions = []\n",
    "        \n",
    "        # Index any remaining documents\n",
    "        if actions:\n",
    "            helpers.bulk(self.config.opensearch, actions)\n",
    "    \n",
    "    def semantic_search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for relevant documents using embeddings\"\"\"\n",
    "        query_embedding = self.get_embeddings(query)\n",
    "        \n",
    "        script_query = {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_embedding}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.config.opensearch.search(\n",
    "            index=self.index_name,\n",
    "            body={\n",
    "                \"size\": k,\n",
    "                \"query\": script_query,\n",
    "                \"_source\": [\"content\", \"metadata\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return [hit['_source'] for hit in response['hits']['hits']]\n",
    "    \n",
    "    def generate_response(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Generate response using Claude 3.5 Sonnet\"\"\"\n",
    "        context_str = \"\\n\\n\".join([doc['content'] for doc in context])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question. \n",
    "        If you cannot answer the question based on the context, say so.\n",
    "        \n",
    "        Context:\n",
    "        {context_str}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        request_body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = self.config.bedrock.invoke_model(\n",
    "            modelId=self.llm_model_id,\n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['content'][0]['text']\n",
    "    \n",
    "    def query(self, query: str, k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Complete RAG pipeline\n",
    "        \n",
    "        Args:\n",
    "            query: Natural language question\n",
    "            k: Number of context documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - query: Original question\n",
    "            - context: Retrieved relevant documents\n",
    "            - response: Generated answer\n",
    "        \"\"\"\n",
    "        # Get relevant documents\n",
    "        context = self.semantic_search(query, k)\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.generate_response(query, context)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"context\": context,\n",
    "            \"response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage\n",
    "def test_rag_system():\n",
    "    # Initialize AWS configuration\n",
    "    config = AWSConfig()\n",
    "    \n",
    "    # Create RAG instance with test index\n",
    "    rag = BaselineRAG(config, index_name=\"test-rag-documents\")\n",
    "    \n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        {\n",
    "            \"content\": \"Machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from data.\",\n",
    "            \"metadata\": {\"source\": \"test\", \"topic\": \"ML\"}\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Deep learning is a type of machine learning that uses neural networks with multiple layers.\",\n",
    "            \"metadata\": {\"source\": \"test\", \"topic\": \"DL\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Ingest documents\n",
    "    print(\"Ingesting documents...\")\n",
    "    rag.ingest_documents(documents)\n",
    "    \n",
    "    # Test query\n",
    "    print(\"\\nTesting query...\")\n",
    "    result = rag.query(\"What is machine learning?\")\n",
    "    \n",
    "    print(\"\\nResponse:\", result['response'])\n",
    "    print(\"\\nContext used:\")\n",
    "    for doc in result['context']:\n",
    "        print(f\"- {doc['content']}\")\n",
    "        print(f\"  Metadata: {doc['metadata']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_rag_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
