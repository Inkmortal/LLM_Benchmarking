{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion for Baseline RAG\n",
    "\n",
    "This notebook demonstrates document ingestion for the baseline RAG system using Langchain's document processing utilities.\n",
    "\n",
    "## Features\n",
    "- Robust document loading with specialized loaders for different file types\n",
    "- Smart text splitting that preserves context\n",
    "- Metadata preservation and enhancement\n",
    "- Batch processing for large document sets\n",
    "\n",
    "## Usage\n",
    "1. Load source material (text files, PDFs, etc.)\n",
    "2. Process and chunk documents\n",
    "3. Ingest into RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_document(doc: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate document content and metadata.\n",
    "    \n",
    "    Args:\n",
    "        doc: Document to validate\n",
    "        \n",
    "    Returns:\n",
    "        True if document is valid, False otherwise\n",
    "    \"\"\"\n",
    "    # Check for required fields\n",
    "    if not doc.get('content'):\n",
    "        print(\"Document missing content\")\n",
    "        return False\n",
    "    \n",
    "    # Check content is not empty or just whitespace\n",
    "    if not doc['content'].strip():\n",
    "        print(\"Document content is empty or whitespace\")\n",
    "        return False\n",
    "    \n",
    "    # Check content length is reasonable\n",
    "    if len(doc['content']) < 10:  # Arbitrary minimum length\n",
    "        print(f\"Document content too short: {len(doc['content'])} chars\")\n",
    "        return False\n",
    "    \n",
    "    # Check metadata exists\n",
    "    if not isinstance(doc.get('metadata'), dict):\n",
    "        print(\"Document missing metadata dictionary\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(\n",
    "    file_paths: List[str], \n",
    "    chunk_size: int = 500, \n",
    "    chunk_overlap: int = 50, \n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process documents using Langchain's document loaders and text splitter\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to documents\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Initialize text splitter if chunking is enabled\n",
    "    text_splitter = None\n",
    "    if enable_chunking:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    # Map file extensions to loaders\n",
    "    loaders = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.txt': TextLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '*': UnstructuredFileLoader\n",
    "    }\n",
    "    \n",
    "    processed_docs = []\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    \n",
    "    with tqdm_notebook(total=len(file_paths), desc=\"Processing files\") as pbar:\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                # Get appropriate loader\n",
    "                ext = Path(file_path).suffix.lower()\n",
    "                loader_cls = loaders.get(ext, loaders['*'])\n",
    "                \n",
    "                # Load document\n",
    "                loader = loader_cls(file_path)\n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Add file info to metadata\n",
    "                file_metadata = metadata.copy() if metadata else {}\n",
    "                file_metadata.update({\n",
    "                    'source_file': file_path,\n",
    "                    'file_type': ext,\n",
    "                    'file_name': Path(file_path).name\n",
    "                })\n",
    "                \n",
    "                # Add metadata to documents\n",
    "                for doc in docs:\n",
    "                    doc.metadata.update(file_metadata)\n",
    "                \n",
    "                if enable_chunking:\n",
    "                    # Split into chunks\n",
    "                    chunks = text_splitter.split_documents(docs)\n",
    "                    \n",
    "                    # Convert to RAG format\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        chunk_metadata = chunk.metadata.copy()\n",
    "                        chunk_metadata.update({\n",
    "                            'chunk_index': i,\n",
    "                            'total_chunks': len(chunks)\n",
    "                        })\n",
    "                        \n",
    "                        doc_dict = {\n",
    "                            'id': str(uuid.uuid4()),\n",
    "                            'content': chunk.page_content,\n",
    "                            'metadata': chunk_metadata\n",
    "                        }\n",
    "                        \n",
    "                        if validate_document(doc_dict):\n",
    "                            processed_docs.append(doc_dict)\n",
    "                            success_count += 1\n",
    "                        else:\n",
    "                            failure_count += 1\n",
    "                else:\n",
    "                    # Keep documents as is\n",
    "                    for doc in docs:\n",
    "                        doc_dict = {\n",
    "                            'id': str(uuid.uuid4()),\n",
    "                            'content': doc.page_content,\n",
    "                            'metadata': doc.metadata\n",
    "                        }\n",
    "                        \n",
    "                        if validate_document(doc_dict):\n",
    "                            processed_docs.append(doc_dict)\n",
    "                            success_count += 1\n",
    "                        else:\n",
    "                            failure_count += 1\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Status': 'Success',\n",
    "                    'File': Path(file_path).name,\n",
    "                    'Success': success_count,\n",
    "                    'Failed': failure_count\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Status': f'Error: {type(e).__name__}',\n",
    "                    'File': Path(file_path).name,\n",
    "                    'Success': success_count,\n",
    "                    'Failed': failure_count\n",
    "                })\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                failure_count += 1\n",
    "                continue\n",
    "                \n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {success_count} documents\")\n",
    "    print(f\"Failed to process: {failure_count} documents\")\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(\n",
    "    dir_path: str, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    recursive: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process all documents in a directory\n",
    "    \n",
    "    Args:\n",
    "        dir_path: Path to directory\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        recursive: Whether to process subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Get all files\n",
    "    if recursive:\n",
    "        file_paths = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.txt', '.pdf', '.docx')):  # Add more extensions as needed\n",
    "                    file_paths.append(os.path.join(root, file))\n",
    "    else:\n",
    "        file_paths = [\n",
    "            os.path.join(dir_path, f) for f in os.listdir(dir_path)\n",
    "            if f.endswith(('.txt', '.pdf', '.docx'))\n",
    "        ]\n",
    "    \n",
    "    return process_documents(\n",
    "        file_paths, \n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        enable_chunking=enable_chunking,\n",
    "        metadata=metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_documents(\n",
    "    source_path: str, \n",
    "    rag_system: Any, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    batch_size: int = 100,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    \"\"\"Ingest documents from a file or directory into RAG system\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to file or directory\n",
    "        rag_system: Any RAG system with ingest_documents method\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        batch_size: Number of documents to process in each batch\n",
    "        max_retries: Maximum number of retry attempts for failed batches\n",
    "    \"\"\"\n",
    "    # Process documents\n",
    "    if os.path.isfile(source_path):\n",
    "        documents = process_documents(\n",
    "            [source_path], \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            enable_chunking=enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    elif os.path.isdir(source_path):\n",
    "        documents = process_directory(\n",
    "            source_path, \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            enable_chunking=enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source path: {source_path}\")\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No valid documents to ingest\")\n",
    "        return\n",
    "    \n",
    "    # Track success/failure counts\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    \n",
    "    # Ingest in batches with retry logic\n",
    "    total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    with tqdm_notebook(total=total_batches, desc=\"Ingesting documents\") as pbar:\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            batch_num = (i//batch_size) + 1\n",
    "            \n",
    "            # Retry logic for failed batches\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    rag_system._store_documents(batch, batch_size=batch_size)\n",
    "                    success_count += len(batch)\n",
    "                    pbar.set_postfix({\n",
    "                        'Status': 'Success',\n",
    "                        'Batch': f\"{batch_num}/{total_batches}\",\n",
    "                        'Success': success_count,\n",
    "                        'Failed': failure_count\n",
    "                    })\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        failure_count += len(batch)\n",
    "                        pbar.set_postfix({\n",
    "                            'Status': f'Error: {type(e).__name__}',\n",
    "                            'Batch': f\"{batch_num}/{total_batches}\",\n",
    "                            'Success': success_count,\n",
    "                            'Failed': failure_count\n",
    "                        })\n",
    "                        print(f\"Error ingesting batch {batch_num} after {max_retries} attempts: {str(e)}\")\n",
    "                    else:\n",
    "                        pbar.set_postfix({\n",
    "                            'Status': f'Retry {attempt+1}/{max_retries}',\n",
    "                            'Batch': f\"{batch_num}/{total_batches}\",\n",
    "                            'Success': success_count,\n",
    "                            'Failed': failure_count\n",
    "                        })\n",
    "                        continue\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nIngestion complete:\")\n",
    "    print(f\"Successfully ingested: {success_count} documents\")\n",
    "    print(f\"Failed to ingest: {failure_count} documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
