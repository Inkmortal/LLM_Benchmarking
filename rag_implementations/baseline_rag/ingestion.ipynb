{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion for Baseline RAG\n",
    "\n",
    "This notebook demonstrates document ingestion for the baseline RAG system using Langchain's document processing utilities.\n",
    "\n",
    "## Features\n",
    "- Robust document loading with specialized loaders for different file types\n",
    "- Smart text splitting that preserves context\n",
    "- Metadata preservation and enhancement\n",
    "- Batch processing for large document sets\n",
    "\n",
    "## Usage\n",
    "1. Load source material (text files, PDFs, etc.)\n",
    "2. Process and chunk documents\n",
    "3. Ingest into RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(\n",
    "    file_paths: List[str], \n",
    "    chunk_size: int = 500, \n",
    "    chunk_overlap: int = 50, \n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process documents using Langchain's document loaders and text splitter\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to documents\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Initialize text splitter if chunking is enabled\n",
    "    text_splitter = None\n",
    "    if enable_chunking:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    # Map file extensions to loaders\n",
    "    loaders = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.txt': TextLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '*': UnstructuredFileLoader\n",
    "    }\n",
    "    \n",
    "    processed_docs = []\n",
    "    with tqdm_notebook(total=len(file_paths), desc=\"Processing files\") as pbar:\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                # Get appropriate loader\n",
    "                ext = Path(file_path).suffix.lower()\n",
    "                loader_cls = loaders.get(ext, loaders['*'])\n",
    "                \n",
    "                # Load document\n",
    "                loader = loader_cls(file_path)\n",
    "                docs = loader.load()\n",
    "                \n",
    "                # Add file info to metadata\n",
    "                file_metadata = metadata.copy() if metadata else {}\n",
    "                file_metadata.update({\n",
    "                    'source_file': file_path,\n",
    "                    'file_type': ext,\n",
    "                    'file_name': Path(file_path).name\n",
    "                })\n",
    "                \n",
    "                # Add metadata to documents\n",
    "                for doc in docs:\n",
    "                    doc.metadata.update(file_metadata)\n",
    "                \n",
    "                if enable_chunking:\n",
    "                    # Split into chunks\n",
    "                    chunks = text_splitter.split_documents(docs)\n",
    "                    \n",
    "                    # Convert to RAG format\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        chunk_metadata = chunk.metadata.copy()\n",
    "                        chunk_metadata.update({\n",
    "                            'chunk_index': i,\n",
    "                            'total_chunks': len(chunks)\n",
    "                        })\n",
    "                        \n",
    "                        processed_docs.append({\n",
    "                            'content': chunk.page_content,\n",
    "                            'metadata': chunk_metadata\n",
    "                        })\n",
    "                else:\n",
    "                    # Keep documents as is\n",
    "                    for doc in docs:\n",
    "                        processed_docs.append({\n",
    "                            'content': doc.page_content,\n",
    "                            'metadata': doc.metadata\n",
    "                        })\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Status': 'Success',\n",
    "                    'File': Path(file_path).name\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Status': f'Error: {type(e).__name__}',\n",
    "                    'File': Path(file_path).name\n",
    "                })\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            pbar.update(1)\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(\n",
    "    dir_path: str, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    recursive: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process all documents in a directory\n",
    "    \n",
    "    Args:\n",
    "        dir_path: Path to directory\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        recursive: Whether to process subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Get all files\n",
    "    if recursive:\n",
    "        file_paths = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.txt', '.pdf', '.docx')):  # Add more extensions as needed\n",
    "                    file_paths.append(os.path.join(root, file))\n",
    "    else:\n",
    "        file_paths = [\n",
    "            os.path.join(dir_path, f) for f in os.listdir(dir_path)\n",
    "            if f.endswith(('.txt', '.pdf', '.docx'))\n",
    "        ]\n",
    "    \n",
    "    return process_documents(\n",
    "        file_paths, \n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        enable_chunking=enable_chunking,\n",
    "        metadata=metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_documents(\n",
    "    source_path: str, \n",
    "    rag_system: Any, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    batch_size: int = 100\n",
    "):\n",
    "    \"\"\"Ingest documents from a file or directory into RAG system\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to file or directory\n",
    "        rag_system: Any RAG system with ingest_documents method\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    # Process documents\n",
    "    if os.path.isfile(source_path):\n",
    "        documents = process_documents(\n",
    "            [source_path], \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            enable_chunking=enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    elif os.path.isdir(source_path):\n",
    "        documents = process_directory(\n",
    "            source_path, \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            enable_chunking=enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source path: {source_path}\")\n",
    "    \n",
    "    # Ingest in batches\n",
    "    total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    with tqdm_notebook(total=total_batches, desc=\"Ingesting documents\") as pbar:\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            try:\n",
    "                rag_system._store_documents(batch, batch_size=batch_size)\n",
    "                pbar.set_postfix({\n",
    "                    'Status': 'Success',\n",
    "                    'Batch': f\"{(i//batch_size)+1}/{total_batches}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Status': f'Error: {type(e).__name__}',\n",
    "                    'Batch': f\"{(i//batch_size)+1}/{total_batches}\"\n",
    "                })\n",
    "                print(f\"Error ingesting batch {(i//batch_size)+1}: {str(e)}\")\n",
    "                continue\n",
    "            pbar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
