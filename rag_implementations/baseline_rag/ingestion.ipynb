{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion for Baseline RAG\n",
    "\n",
    "This notebook handles document preprocessing and ingestion for the baseline RAG system. It can process any text-based source material and prepare it for RAG.\n",
    "\n",
    "## Features\n",
    "- Text extraction and cleaning\n",
    "- Document chunking with configurable size and overlap\n",
    "- Metadata preservation\n",
    "- Batch processing for large document sets\n",
    "\n",
    "## Usage\n",
    "1. Load source material (text files, PDFs, etc.)\n",
    "2. Process and chunk documents\n",
    "3. Ingest into RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Generator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our RAG implementation\n",
    "from implementation import AWSConfig, BaselineRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DocumentPreprocessor:\n",
    "    \"\"\"Handles document preprocessing for RAG ingestion\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"Initialize preprocessor\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Maximum number of characters per chunk\n",
    "            chunk_overlap: Number of characters to overlap between chunks\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text content\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # TODO: Add more cleaning steps as needed\n",
    "        return text\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            # Find the end of the chunk\n",
    "            end = start + self.chunk_size\n",
    "            \n",
    "            # If we're not at the end of the text, try to break at a sentence\n",
    "            if end < len(text):\n",
    "                # Look for sentence boundaries (.!?) within the last 100 chars of the chunk\n",
    "                search_region = text[end-100:end]\n",
    "                last_period = max(\n",
    "                    search_region.rfind('. '),\n",
    "                    search_region.rfind('! '),\n",
    "                    search_region.rfind('? ')\n",
    "                )\n",
    "                \n",
    "                if last_period != -1:\n",
    "                    end = end - (100 - last_period - 2)  # -2 for the punctuation and space\n",
    "            \n",
    "            # Extract the chunk\n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:  # Only add non-empty chunks\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            # Move the start position, accounting for overlap\n",
    "            start = end - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_document(self, content: str, metadata: Optional[Dict] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a single document\n",
    "        \n",
    "        Args:\n",
    "            content: Document text content\n",
    "            metadata: Optional metadata to preserve\n",
    "            \n",
    "        Returns:\n",
    "            List of processed chunks with metadata\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        cleaned_text = self.clean_text(content)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self.chunk_text(cleaned_text)\n",
    "        \n",
    "        # Prepare documents for ingestion\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Create metadata for chunk\n",
    "            chunk_metadata = metadata.copy() if metadata else {}\n",
    "            chunk_metadata.update({\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            })\n",
    "            \n",
    "            documents.append({\n",
    "                'content': chunk,\n",
    "                'metadata': chunk_metadata\n",
    "            })\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def process_text_file(self, file_path: str, metadata: Optional[Dict] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a text file\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to text file\n",
    "            metadata: Optional metadata to preserve\n",
    "            \n",
    "        Returns:\n",
    "            List of processed chunks with metadata\n",
    "        \"\"\"\n",
    "        # Add file info to metadata\n",
    "        file_metadata = metadata.copy() if metadata else {}\n",
    "        file_metadata.update({\n",
    "            'source_file': file_path,\n",
    "            'file_type': 'text'\n",
    "        })\n",
    "        \n",
    "        # Read and process file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        return self.process_document(content, file_metadata)\n",
    "    \n",
    "    def process_directory(self, dir_path: str, metadata: Optional[Dict] = None) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Process all text files in a directory\n",
    "        \n",
    "        Args:\n",
    "            dir_path: Path to directory\n",
    "            metadata: Optional metadata to preserve\n",
    "            \n",
    "        Yields:\n",
    "            Processed chunks with metadata\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):  # TODO: Add support for more file types\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    \n",
    "                    try:\n",
    "                        documents = self.process_text_file(file_path, metadata)\n",
    "                        for doc in documents:\n",
    "                            yield doc\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def ingest_documents(source_path: str, rag_system: BaselineRAG, metadata: Optional[Dict] = None, batch_size: int = 100):\n",
    "    \"\"\"Ingest documents from a file or directory into RAG system\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to file or directory\n",
    "        rag_system: Initialized RAG system\n",
    "        metadata: Optional metadata to preserve\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DocumentPreprocessor()\n",
    "    \n",
    "    # Process and ingest documents\n",
    "    if os.path.isfile(source_path):\n",
    "        # Process single file\n",
    "        documents = preprocessor.process_text_file(source_path, metadata)\n",
    "        rag_system.ingest_documents(documents, batch_size=batch_size)\n",
    "    \n",
    "    elif os.path.isdir(source_path):\n",
    "        # Process directory\n",
    "        batch = []\n",
    "        \n",
    "        for doc in preprocessor.process_directory(source_path, metadata):\n",
    "            batch.append(doc)\n",
    "            \n",
    "            if len(batch) >= batch_size:\n",
    "                rag_system.ingest_documents(batch, batch_size=batch_size)\n",
    "                batch = []\n",
    "        \n",
    "        # Ingest any remaining documents\n",
    "        if batch:\n",
    "            rag_system.ingest_documents(batch, batch_size=batch_size)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source path: {source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example usage\n",
    "def test_ingestion():\n",
    "    # Create test documents\n",
    "    os.makedirs('test_docs', exist_ok=True)\n",
    "    \n",
    "    with open('test_docs/doc1.txt', 'w') as f:\n",
    "        f.write(\"\"\"Machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from data.\n",
    "        Deep learning is a type of machine learning that uses neural networks with multiple layers.\n",
    "        Reinforcement learning is another type of machine learning where agents learn by interacting with an environment.\"\"\")\n",
    "    \n",
    "    with open('test_docs/doc2.txt', 'w') as f:\n",
    "        f.write(\"\"\"Natural Language Processing (NLP) is a field of AI that focuses on interactions between computers and human language.\n",
    "        Common NLP tasks include text classification, named entity recognition, and machine translation.\"\"\")\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    config = AWSConfig()\n",
    "    rag = BaselineRAG(config, index_name=\"test-rag-documents\")\n",
    "    \n",
    "    # Test ingestion\n",
    "    print(\"Ingesting documents...\")\n",
    "    ingest_documents(\n",
    "        'test_docs',\n",
    "        rag,\n",
    "        metadata={'dataset': 'test', 'domain': 'AI/ML'},\n",
    "        batch_size=2\n",
    "    )\n",
    "    \n",
    "    # Test query\n",
    "    print(\"\\nTesting query...\")\n",
    "    result = rag.query(\"What is machine learning and deep learning?\")\n",
    "    \n",
    "    print(\"\\nResponse:\", result['response'])\n",
    "    print(\"\\nContext used:\")\n",
    "    for doc in result['context']:\n",
    "        print(f\"- {doc['content']}\")\n",
    "        print(f\"  Metadata: {doc['metadata']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    import shutil\n",
    "    shutil.rmtree('test_docs')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ingestion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
