{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion for Baseline RAG\n",
    "\n",
    "This notebook demonstrates document ingestion for the baseline RAG system using Langchain's document processing utilities.\n",
    "\n",
    "## Features\n",
    "- Robust document loading with specialized loaders for different file types\n",
    "- Smart text splitting that preserves context\n",
    "- Metadata preservation and enhancement\n",
    "- Batch processing for large document sets\n",
    "\n",
    "## Usage\n",
    "1. Load source material (text files, PDFs, etc.)\n",
    "2. Process and chunk documents\n",
    "3. Ingest into RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(\n",
    "    file_paths: List[str], \n",
    "    chunk_size: int = 500, \n",
    "    chunk_overlap: int = 50, \n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process documents using Langchain's document loaders and text splitter\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to documents\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Initialize text splitter if chunking is enabled\n",
    "    text_splitter = None\n",
    "    if enable_chunking:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    # Map file extensions to loaders\n",
    "    loaders = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.txt': TextLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '*': UnstructuredFileLoader\n",
    "    }\n",
    "    \n",
    "    processed_docs = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Get appropriate loader\n",
    "            ext = Path(file_path).suffix.lower()\n",
    "            loader_cls = loaders.get(ext, loaders['*'])\n",
    "            \n",
    "            # Load document\n",
    "            loader = loader_cls(file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Add file info to metadata\n",
    "            file_metadata = metadata.copy() if metadata else {}\n",
    "            file_metadata.update({\n",
    "                'source_file': file_path,\n",
    "                'file_type': ext,\n",
    "                'file_name': Path(file_path).name\n",
    "            })\n",
    "            \n",
    "            # Add metadata to documents\n",
    "            for doc in docs:\n",
    "                doc.metadata.update(file_metadata)\n",
    "            \n",
    "            if enable_chunking:\n",
    "                # Split into chunks\n",
    "                chunks = text_splitter.split_documents(docs)\n",
    "                \n",
    "                # Convert to RAG format\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_metadata = chunk.metadata.copy()\n",
    "                    chunk_metadata.update({\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks)\n",
    "                    })\n",
    "                    \n",
    "                    processed_docs.append({\n",
    "                        'content': chunk.page_content,\n",
    "                        'metadata': chunk_metadata\n",
    "                    })\n",
    "            else:\n",
    "                # Keep documents as is\n",
    "                for doc in docs:\n",
    "                    processed_docs.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': doc.metadata\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(\n",
    "    dir_path: str, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    recursive: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process all documents in a directory\n",
    "    \n",
    "    Args:\n",
    "        dir_path: Path to directory\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        recursive: Whether to process subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Get all files\n",
    "    if recursive:\n",
    "        file_paths = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.txt', '.pdf', '.docx')):  # Add more extensions as needed\n",
    "                    file_paths.append(os.path.join(root, file))\n",
    "    else:\n",
    "        file_paths = [\n",
    "            os.path.join(dir_path, f) for f in os.listdir(dir_path)\n",
    "            if f.endswith(('.txt', '.pdf', '.docx'))\n",
    "        ]\n",
    "    \n",
    "    return process_documents(\n",
    "        file_paths, \n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        enable_chunking=enable_chunking,\n",
    "        metadata=metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_documents(\n",
    "    source_path: str, \n",
    "    rag_system: Any, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    batch_size: int = 100\n",
    "):\n",
    "    \"\"\"Ingest documents from a file or directory into RAG system\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to file or directory\n",
    "        rag_system: Any RAG system with ingest_documents method\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    # Process documents\n",
    "    if os.path.isfile(source_path):\n",
    "        documents = process_documents(\n",
    "            [source_path], \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            enable_chunking=enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    elif os.path.isdir(source_path):\n",
    "        documents = process_directory(\n",
    "            source_path, \n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            enable_chunking=enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source path: {source_path}\")\n",
    "    \n",
    "    # Ingest in batches\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        rag_system._store_documents(batch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def test_ingestion():\n",
    "    \"\"\"Test the document ingestion pipeline\"\"\"\n",
    "    # Create test documents\n",
    "    os.makedirs('test_docs', exist_ok=True)\n",
    "    \n",
    "    with open('test_docs/doc1.txt', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "        Machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from data.\n",
    "        Deep learning is a type of machine learning that uses neural networks with multiple layers.\n",
    "        Reinforcement learning is another type of machine learning where agents learn by interacting with an environment.\n",
    "        \"\"\")\n",
    "    \n",
    "    with open('test_docs/doc2.txt', 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "        Natural Language Processing (NLP) is a field of AI that focuses on interactions between computers and human language.\n",
    "        Common NLP tasks include text classification, named entity recognition, and machine translation.\n",
    "        \"\"\")\n",
    "    \n",
    "    # Import our RAG implementation\n",
    "    from utils.notebook_utils.importable import notebook_to_module\n",
    "    baseline_rag = notebook_to_module('implementation.ipynb')\n",
    "    BaselineRAG = baseline_rag.BaselineRAG\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = BaselineRAG(index_name=\"test-rag-documents\")\n",
    "    \n",
    "    # Test ingestion\n",
    "    print(\"Ingesting documents...\")\n",
    "    ingest_documents(\n",
    "        'test_docs',\n",
    "        rag,\n",
    "        metadata={'dataset': 'test', 'domain': 'AI/ML'},\n",
    "        batch_size=2\n",
    "    )\n",
    "    \n",
    "    # Test query\n",
    "    print(\"\\nTesting query...\")\n",
    "    result = rag.query(\"What is machine learning and deep learning?\")\n",
    "    \n",
    "    print(\"\\nResponse:\", result['response'])\n",
    "    print(\"\\nContext used:\")\n",
    "    for doc in result['context']:\n",
    "        print(f\"- {doc['content']}\")\n",
    "        print(f\"  Metadata: {doc['metadata']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    import shutil\n",
    "    shutil.rmtree('test_docs')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ingestion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
