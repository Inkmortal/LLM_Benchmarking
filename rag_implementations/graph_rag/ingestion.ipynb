{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Document Ingestion\n",
    "\n",
    "This notebook handles document processing and graph construction for the GraphRAG implementation.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "1. Load and preprocess documents\n",
    "2. Extract entities and relationships\n",
    "3. Construct knowledge graph\n",
    "4. Store document vectors\n",
    "5. Update graph indices\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- Batch size for processing\n",
    "- Entity extraction settings\n",
    "- Relationship confidence thresholds\n",
    "- Graph storage parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.notebook_utils.document_utils import load_documents, process_documents\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def ingest_documents(\n",
    "    source_dir: str,\n",
    "    rag_instance: Any,\n",
    "    metadata: Optional[Dict[str, Any]] = None,\n",
    "    batch_size: int = 100\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process documents and construct graph representation.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Directory containing source documents\n",
    "        rag_instance: GraphRAG instance\n",
    "        metadata: Optional metadata to attach to documents\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    # Load documents\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents(source_dir)\n",
    "    \n",
    "    # Process in batches\n",
    "    total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size), total=total_batches):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        \n",
    "        # Process batch\n",
    "        processed_docs = process_documents(\n",
    "            batch,\n",
    "            chunk_size=rag_instance.chunk_size,\n",
    "            chunk_overlap=rag_instance.chunk_overlap,\n",
    "            enable_chunking=rag_instance.enable_chunking\n",
    "        )\n",
    "        \n",
    "        # Process each document\n",
    "        for doc in processed_docs:\n",
    "            # Add metadata\n",
    "            if metadata:\n",
    "                doc[\"metadata\"].update(metadata)\n",
    "            \n",
    "            # Extract graph data\n",
    "            graph_data = rag_instance._extract_entities_relations(doc[\"content\"])\n",
    "            \n",
    "            # Store in graph database\n",
    "            rag_instance._store_graph_data(doc[\"id\"], graph_data)\n",
    "            \n",
    "            # Get document embedding\n",
    "            embedding = rag_instance.llm.get_embedding(doc[\"content\"])\n",
    "            \n",
    "            # Store in vector index\n",
    "            rag_instance.opensearch.index(\n",
    "                index=rag_instance.index_name,\n",
    "                id=doc[\"id\"],\n",
    "                body={\n",
    "                    \"content\": doc[\"content\"],\n",
    "                    \"vector\": embedding,\n",
    "                    \"metadata\": doc[\"metadata\"]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    print(f\"Processed {len(documents)} documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
