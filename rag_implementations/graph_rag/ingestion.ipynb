{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Document Ingestion\n",
    "\n",
    "This notebook handles document processing and graph construction for the GraphRAG implementation.\n",
    "\n",
    "## Process Overview\n",
    "1. Load and preprocess documents using Langchain\n",
    "2. Extract entities and relationships\n",
    "3. Construct knowledge graph\n",
    "4. Store document vectors\n",
    "5. Update graph indices\n",
    "\n",
    "## Configuration\n",
    "- Document processing settings\n",
    "- Entity extraction parameters\n",
    "- Graph storage configuration\n",
    "- Vector storage settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import random\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import components\n",
    "from rag_implementations.graph_rag.components.document_processor import DocumentProcessor\n",
    "from rag_implementations.graph_rag.components.graph_store import GraphStore\n",
    "from rag_implementations.graph_rag.components.vector_store import VectorStore\n",
    "from rag_implementations.graph_rag.components.response_generator import ResponseGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(\n",
    "    text: str,\n",
    "    model_id: str = \"cohere.embed-english-v3\",\n",
    "    max_retries: int = 5,\n",
    "    min_delay: float = 1.0,\n",
    "    max_delay: float = 60.0\n",
    ") -> Optional[List[float]]:\n",
    "    \"\"\"Generate embeddings using Cohere model with retry.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model_id: Bedrock model ID\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        min_delay: Minimum delay between retries in seconds\n",
    "        max_delay: Maximum delay between retries in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Embedding vector or None if error\n",
    "    \"\"\"\n",
    "    bedrock = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    request_body = {\n",
    "        \"texts\": [text],\n",
    "        \"input_type\": \"search_document\"\n",
    "    }\n",
    "    \n",
    "    last_exception = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = bedrock.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps(request_body)\n",
    "            )\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            return response_body['embeddings'][0]\n",
    "            \n",
    "        except ClientError as e:\n",
    "            last_exception = e\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Max retries exceeded for text: {text[:100]}...\")\n",
    "                    return None\n",
    "                # Exponential backoff with jitter\n",
    "                delay = min(\n",
    "                    max_delay,\n",
    "                    min_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                )\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Error getting embeddings: {str(e)}\")\n",
    "                print(f\"Text length: {len(text)} chars, {len(text.split())} words\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings: {str(e)}\")\n",
    "            print(f\"Text length: {len(text)} chars, {len(text.split())} words\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Failed after {max_retries} attempts: {str(last_exception)}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_documents(\n",
    "    source_path: str,\n",
    "    rag_instance: Any,\n",
    "    metadata: Optional[Dict] = None,\n",
    "    batch_size: int = 100\n",
    ") -> None:\n",
    "    \"\"\"Process documents and construct graph representation.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to file or directory\n",
    "        rag_instance: GraphRAG instance\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    # Process documents\n",
    "    if os.path.isfile(source_path):\n",
    "        documents = rag_instance.doc_processor.process_files(\n",
    "            [source_path],\n",
    "            metadata=metadata\n",
    "        )\n",
    "    elif os.path.isdir(source_path):\n",
    "        documents = rag_instance.doc_processor.process_directory(\n",
    "            source_path,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source path: {source_path}\")\n",
    "    \n",
    "    # Track success/failure counts\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Processing documents\"):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        \n",
    "        # Prepare batch for storage\n",
    "        vector_docs = []\n",
    "        graph_docs = []\n",
    "        \n",
    "        # Process each document\n",
    "        for doc in batch:\n",
    "            try:\n",
    "                # Generate unique ID\n",
    "                doc_id = str(uuid.uuid4())\n",
    "                \n",
    "                # Get document embedding first\n",
    "                embedding = get_embedding(doc[\"content\"])\n",
    "                if embedding is None:\n",
    "                    print(f\"Skipping document {doc_id} due to embedding error\")\n",
    "                    failure_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Prepare document for vector store\n",
    "                vector_docs.append({\n",
    "                    'content': doc[\"content\"],\n",
    "                    'vector': embedding,\n",
    "                    'metadata': doc[\"metadata\"]\n",
    "                })\n",
    "                \n",
    "                # Prepare document for graph store\n",
    "                graph_docs.append({\n",
    "                    'doc_id': doc_id,\n",
    "                    'content': doc[\"content\"],\n",
    "                    'metadata': doc[\"metadata\"],\n",
    "                    'graph_data': doc[\"graph_data\"]\n",
    "                })\n",
    "                \n",
    "                success_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document: {str(e)}\")\n",
    "                failure_count += 1\n",
    "                continue\n",
    "        \n",
    "        # Store batch in vector store\n",
    "        if vector_docs:\n",
    "            try:\n",
    "                rag_instance.vector_store.store_documents(vector_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing vectors: {str(e)}\")\n",
    "                failure_count += len(vector_docs)\n",
    "                success_count -= len(vector_docs)\n",
    "                continue\n",
    "        \n",
    "        # Store batch in graph store\n",
    "        if graph_docs:\n",
    "            try:\n",
    "                for doc in graph_docs:\n",
    "                    rag_instance.graph_store.store_document(\n",
    "                        doc_id=doc['doc_id'],\n",
    "                        content=doc['content'],\n",
    "                        metadata=doc['metadata'],\n",
    "                        graph_data=doc['graph_data']\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Error storing graph data: {str(e)}\")\n",
    "                failure_count += len(graph_docs)\n",
    "                success_count -= len(graph_docs)\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\nIngestion complete:\")\n",
    "    print(f\"Successfully processed: {success_count} documents\")\n",
    "    print(f\"Failed to process: {failure_count} documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
