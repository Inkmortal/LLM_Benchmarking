{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Document Ingestion\n",
    "\n",
    "This notebook handles document processing and graph construction for the GraphRAG implementation.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "1. Load and preprocess documents using Langchain\n",
    "2. Extract entities and relationships\n",
    "3. Construct knowledge graph\n",
    "4. Store document vectors\n",
    "5. Update graph indices\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- Batch size for processing\n",
    "- Entity extraction settings\n",
    "- Relationship confidence thresholds\n",
    "- Graph storage parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_documents(\n",
    "    file_paths: List[str], \n",
    "    chunk_size: int = 500, \n",
    "    chunk_overlap: int = 50, \n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process documents using Langchain's document loaders and text splitter\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to documents\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Initialize text splitter if chunking is enabled\n",
    "    text_splitter = None\n",
    "    if enable_chunking:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    # Map file extensions to loaders\n",
    "    loaders = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.txt': TextLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '*': UnstructuredFileLoader\n",
    "    }\n",
    "    \n",
    "    processed_docs = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Get appropriate loader\n",
    "            ext = Path(file_path).suffix.lower()\n",
    "            loader_cls = loaders.get(ext, loaders['*'])\n",
    "            \n",
    "            # Load document\n",
    "            loader = loader_cls(file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Add file info to metadata\n",
    "            file_metadata = metadata.copy() if metadata else {}\n",
    "            file_metadata.update({\n",
    "                'source_file': file_path,\n",
    "                'file_type': ext,\n",
    "                'file_name': Path(file_path).name\n",
    "            })\n",
    "            \n",
    "            # Add metadata to documents\n",
    "            for doc in docs:\n",
    "                doc.metadata.update(file_metadata)\n",
    "            \n",
    "            if enable_chunking:\n",
    "                # Split into chunks\n",
    "                chunks = text_splitter.split_documents(docs)\n",
    "                \n",
    "                # Convert to RAG format\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_metadata = chunk.metadata.copy()\n",
    "                    chunk_metadata.update({\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks)\n",
    "                    })\n",
    "                    \n",
    "                    processed_docs.append({\n",
    "                        'content': chunk.page_content,\n",
    "                        'metadata': chunk_metadata\n",
    "                    })\n",
    "            else:\n",
    "                # Keep documents as is\n",
    "                for doc in docs:\n",
    "                    processed_docs.append({\n",
    "                        'content': doc.page_content,\n",
    "                        'metadata': doc.metadata\n",
    "                    })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "def process_directory(\n",
    "    dir_path: str, \n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    "    enable_chunking: bool = True,\n",
    "    metadata: Optional[Dict] = None, \n",
    "    recursive: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process all documents in a directory\n",
    "    \n",
    "    Args:\n",
    "        dir_path: Path to directory\n",
    "        chunk_size: Maximum number of characters per chunk\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "        enable_chunking: Whether to split documents into chunks\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        recursive: Whether to process subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        List of processed documents ready for RAG ingestion\n",
    "    \"\"\"\n",
    "    # Get all files\n",
    "    if recursive:\n",
    "        file_paths = []\n",
    "        for root, _, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.txt', '.pdf', '.docx')):  # Add more extensions as needed\n",
    "                    file_paths.append(os.path.join(root, file))\n",
    "    else:\n",
    "        file_paths = [\n",
    "            os.path.join(dir_path, f) for f in os.listdir(dir_path)\n",
    "            if f.endswith(('.txt', '.pdf', '.docx'))\n",
    "        ]\n",
    "    \n",
    "    return process_documents(\n",
    "        file_paths, \n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        enable_chunking=enable_chunking,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "def ingest_documents(\n",
    "    source_path: str, \n",
    "    rag_instance: Any, \n",
    "    metadata: Optional[Dict] = None, \n",
    "    batch_size: int = 100\n",
    ") -> None:\n",
    "    \"\"\"Ingest documents into GraphRAG system\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to file or directory\n",
    "        rag_instance: GraphRAG instance\n",
    "        metadata: Optional metadata to add to all documents\n",
    "        batch_size: Number of documents to process in each batch\n",
    "    \"\"\"\n",
    "    # Process documents\n",
    "    if os.path.isfile(source_path):\n",
    "        documents = process_documents(\n",
    "            [source_path], \n",
    "            chunk_size=rag_instance.chunk_size,\n",
    "            chunk_overlap=rag_instance.chunk_overlap,\n",
    "            enable_chunking=rag_instance.enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    elif os.path.isdir(source_path):\n",
    "        documents = process_directory(\n",
    "            source_path, \n",
    "            chunk_size=rag_instance.chunk_size,\n",
    "            chunk_overlap=rag_instance.chunk_overlap,\n",
    "            enable_chunking=rag_instance.enable_chunking,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid source path: {source_path}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Processing documents\"):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        \n",
    "        # Process each document\n",
    "        for doc in batch:\n",
    "            # Add document ID if not present\n",
    "            if 'id' not in doc:\n",
    "                doc['id'] = f\"doc_{i}\"\n",
    "                \n",
    "            # Extract graph data\n",
    "            graph_data = rag_instance._extract_entities_relations(doc[\"content\"])\n",
    "            \n",
    "            # Store in graph database\n",
    "            rag_instance._store_graph_data(doc[\"id\"], graph_data)\n",
    "            \n",
    "            # Get document embedding\n",
    "            embedding = rag_instance.llm.get_embedding(doc[\"content\"])\n",
    "            \n",
    "            # Store in vector index\n",
    "            rag_instance.opensearch.index(\n",
    "                index=rag_instance.index_name,\n",
    "                id=doc[\"id\"],\n",
    "                body={\n",
    "                    \"content\": doc[\"content\"],\n",
    "                    \"vector\": embedding,\n",
    "                    \"metadata\": doc[\"metadata\"]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    print(f\"Processed {len(documents)} documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
