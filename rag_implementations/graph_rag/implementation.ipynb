{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Implementation\n",
    "\n",
    "This notebook implements a Graph-based RAG system using Neptune for graph storage and hybrid search combining graph and vector retrieval.\n",
    "\n",
    "## Features\n",
    "- Document processing with entity/relation extraction\n",
    "- Graph storage in Neptune\n",
    "- Vector storage in OpenSearch\n",
    "- Hybrid search combining graph and vector similarity\n",
    "- Context-aware response generation\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Graph Construction\n",
    "- min_entity_freq: Minimum frequency for entity inclusion (default: 2)\n",
    "- max_relation_distance: Maximum token distance for relationships (default: 10)\n",
    "\n",
    "### Hybrid Search\n",
    "- k_graph: Number of graph-based results (default: 5)\n",
    "- k_vector: Number of vector-based results (default: 3)\n",
    "- alpha: Weight for combining scores (default: 0.7)\n",
    "- search_type: Type of vector search ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score (default: None)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum retry attempts (default: 5)\n",
    "- min_delay: Minimum retry delay in seconds (default: 1)\n",
    "- max_delay: Maximum retry delay in seconds (default: 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import components\n",
    "from rag_implementations.graph_rag.components.document_processor import DocumentProcessor\n",
    "from rag_implementations.graph_rag.components.graph_store import GraphStore\n",
    "from rag_implementations.graph_rag.components.vector_store import VectorStore\n",
    "from rag_implementations.graph_rag.components.hybrid_search import HybridSearch\n",
    "from rag_implementations.graph_rag.components.response_generator import ResponseGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAG:\n",
    "    \"\"\"Graph-based RAG implementation using Neptune and OpenSearch.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        # Document processing config\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50,\n",
    "        enable_chunking: bool = True,\n",
    "        # Graph construction config\n",
    "        min_entity_freq: int = 2,\n",
    "        max_relation_distance: int = 10,\n",
    "        # Hybrid search config\n",
    "        k_graph: int = 5,\n",
    "        k_vector: int = 3,\n",
    "        alpha: float = 0.7,\n",
    "        search_type: Literal['script', 'knn'] = 'script',\n",
    "        similarity_threshold: Optional[float] = None,\n",
    "        # OpenSearch config\n",
    "        index_settings: Optional[Dict] = None,\n",
    "        knn_params: Optional[Dict] = None,\n",
    "        # Neptune config\n",
    "        graph_store_config: Optional[Dict] = None,\n",
    "        # API config\n",
    "        max_retries: int = 5,\n",
    "        min_delay: float = 1.0,\n",
    "        max_delay: float = 60.0\n",
    "    ):\n",
    "        \"\"\"Initialize GraphRAG with configuration parameters.\"\"\"\n",
    "        self.index_name = index_name\n",
    "        \n",
    "        # Track component state\n",
    "        self._initialized = False\n",
    "        self.doc_processor = None\n",
    "        self.vector_store = None\n",
    "        self.graph_store = None\n",
    "        self.hybrid_search = None\n",
    "        self.response_generator = None\n",
    "        \n",
    "        try:\n",
    "            # Initialize components in order\n",
    "            self.doc_processor = DocumentProcessor(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                enable_chunking=enable_chunking,\n",
    "                min_entity_freq=min_entity_freq,\n",
    "                max_relation_distance=max_relation_distance\n",
    "            )\n",
    "            \n",
    "            # Vector store first (faster to set up)\n",
    "            self.vector_store = VectorStore(\n",
    "                index_name=index_name,\n",
    "                search_type=search_type,\n",
    "                similarity_threshold=similarity_threshold,\n",
    "                index_settings=index_settings,\n",
    "                knn_params=knn_params\n",
    "            )\n",
    "            \n",
    "            # Graph store with proper error handling\n",
    "            try:\n",
    "                # Use provided config or defaults\n",
    "                default_config = {\n",
    "                    \"cluster_name\": f\"graph-rag-{index_name}\",\n",
    "                    \"enable_audit\": True\n",
    "                }\n",
    "                \n",
    "                # Filter config to only include supported parameters\n",
    "                if graph_store_config:\n",
    "                    graph_config = {\n",
    "                        \"cluster_name\": graph_store_config.get(\"cluster_name\", default_config[\"cluster_name\"]),\n",
    "                        \"enable_audit\": graph_store_config.get(\"enable_audit\", default_config[\"enable_audit\"])\n",
    "                    }\n",
    "                else:\n",
    "                    graph_config = default_config\n",
    "                    \n",
    "                self.graph_store = GraphStore(**graph_config)\n",
    "            except Exception as e:\n",
    "                # Clean up vector store if graph store fails\n",
    "                self.vector_store.cleanup(delete_resources=False)\n",
    "                raise\n",
    "            \n",
    "            # Initialize search after both stores ready\n",
    "            self.hybrid_search = HybridSearch(\n",
    "                graph_store=self.graph_store,\n",
    "                vector_store=self.vector_store,\n",
    "                k_graph=k_graph,\n",
    "                k_vector=k_vector,\n",
    "                alpha=alpha\n",
    "            )\n",
    "            \n",
    "            self.response_generator = ResponseGenerator(\n",
    "                max_retries=max_retries,\n",
    "                min_delay=min_delay,\n",
    "                max_delay=max_delay\n",
    "            )\n",
    "            \n",
    "            self._initialized = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Clean up on initialization failure, but don't delete resources\n",
    "            self.cleanup(delete_resources=False)\n",
    "            raise Exception(f\"Failed to initialize GraphRAG: {str(e)}\") from e\n",
    "    \n",
    "    def ensure_initialized(self):\n",
    "        \"\"\"Ensure all components are properly initialized.\"\"\"\n",
    "        if not self._initialized:\n",
    "            raise RuntimeError(\"GraphRAG not properly initialized\")\n",
    "        if not self.graph_store:\n",
    "            raise RuntimeError(\"Graph store not available\")\n",
    "        if not self.vector_store:\n",
    "            raise RuntimeError(\"Vector store not available\")\n",
    "    \n",
    "    def query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query using graph-augmented retrieval.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing response and context\n",
    "        \"\"\"\n",
    "        self.ensure_initialized()\n",
    "        \n",
    "        try:\n",
    "            # Extract entities from query\n",
    "            query_graph = self.doc_processor._extract_entities_relations(query)\n",
    "            \n",
    "            # Get query embedding from vector store\n",
    "            query_vector = self.vector_store.get_embedding(query)\n",
    "            \n",
    "            # Track query timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Perform hybrid search\n",
    "            try:\n",
    "                search_results = self.hybrid_search.search(\n",
    "                    query_text=query,\n",
    "                    query_vector=query_vector,\n",
    "                    query_entities=query_graph[\"entities\"]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Hybrid search failed: {str(e)}\")\n",
    "                # Fall back to vector search only\n",
    "                search_results = self.vector_store.search(\n",
    "                    query_vector=query_vector,\n",
    "                    k=5\n",
    "                )\n",
    "            \n",
    "            # Get graph context for each result\n",
    "            graph_context = []\n",
    "            for result in search_results:\n",
    "                try:\n",
    "                    doc_entities = self.graph_store.get_document_entities(result[\"id\"])\n",
    "                    doc_relations = self.graph_store.get_document_relations(result[\"id\"])\n",
    "                    \n",
    "                    graph_context.append({\n",
    "                        \"doc_id\": result[\"id\"],\n",
    "                        \"entities\": doc_entities,\n",
    "                        \"relations\": doc_relations\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to get graph context for document {result['id']}: {str(e)}\")\n",
    "                    # Skip this document's graph context\n",
    "                    continue\n",
    "            \n",
    "            # Calculate query time\n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            # Generate response\n",
    "            response = self.response_generator.generate(\n",
    "                query=query,\n",
    "                search_results=search_results,\n",
    "                graph_context=graph_context\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": response,\n",
    "                \"context\": search_results,\n",
    "                \"graph_context\": graph_context,\n",
    "                \"graph_query_time\": query_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to process query: {str(e)}\") from e\n",
    "    \n",
    "    def cleanup(self, delete_resources: bool = False):\n",
    "        \"\"\"Clean up all resources in reverse initialization order.\n",
    "        \n",
    "        Args:\n",
    "            delete_resources: Whether to delete Neptune/OpenSearch resources\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'hybrid_search'):\n",
    "            # Just clear reference\n",
    "            self.hybrid_search = None\n",
    "            \n",
    "        if hasattr(self, 'response_generator'):\n",
    "            self.response_generator = None\n",
    "            \n",
    "        if hasattr(self, 'graph_store'):\n",
    "            try:\n",
    "                self.graph_store.cleanup(delete_resources=delete_resources)\n",
    "            except:\n",
    "                pass  # Best effort cleanup\n",
    "            self.graph_store = None\n",
    "            \n",
    "        if hasattr(self, 'vector_store'):\n",
    "            try:\n",
    "                self.vector_store.cleanup(delete_resources=delete_resources)\n",
    "            except:\n",
    "                pass  # Best effort cleanup\n",
    "            self.vector_store = None\n",
    "            \n",
    "        if hasattr(self, 'doc_processor'):\n",
    "            self.doc_processor = None\n",
    "            \n",
    "        self._initialized = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
