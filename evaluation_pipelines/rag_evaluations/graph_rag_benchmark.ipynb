{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Benchmarking\n",
    "\n",
    "This notebook benchmarks our GraphRAG implementation using the Origin of Covid-19 dataset.\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Graph Construction\n",
    "- min_entity_freq: Minimum frequency for entity inclusion (default: 2)\n",
    "- max_relation_distance: Maximum token distance for relationships (default: 10)\n",
    "\n",
    "### Hybrid Search\n",
    "- k_graph: Number of graph-based results (default: 5)\n",
    "- k_vector: Number of vector-based results (default: 3)\n",
    "- alpha: Weight for combining scores (default: 0.7)\n",
    "- search_type: Type of vector search ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score (default: None)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum retry attempts (default: 5)\n",
    "- min_delay: Minimum retry delay in seconds (default: 1)\n",
    "- max_delay: Maximum retry delay in seconds (default: 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Suppress CUDA warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message=\"Can't initialize NVML\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    save_dataset_info\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module\n",
    "\n",
    "# Import graph RAG components\n",
    "from rag_implementations.graph_rag.components import (\n",
    "    calculate_graph_metrics,\n",
    "    calculate_graph_coverage,\n",
    "    calculate_graph_relevance,\n",
    "    plot_graph_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset Configuration\n",
    "DATASET_NAME = \"OriginOfCovid19Dataset\"\n",
    "DATASET_DIR = project_root / \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "NUM_EVAL_SAMPLES = None  # Set to a number for partial evaluation\n",
    "\n",
    "# Neptune Configuration\n",
    "NEPTUNE_CONFIG = {\n",
    "    \"cluster_name\": \"test-graph-rag-benchmark\",\n",
    "    \"cleanup_enabled\": True,  # For cost control\n",
    "    \"enable_audit\": True,     # For detailed logging\n",
    "    \"max_retries\": 10,        # More retries for initial connection\n",
    "    \"retry_delay\": 5.0        # Start with longer delay\n",
    "}\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_CONFIG = {\n",
    "    # Document processing\n",
    "    \"chunk_size\": 500,  # 500 words ≈ 2000 chars\n",
    "    \"chunk_overlap\": 50,  # 50 words overlap\n",
    "    \"enable_chunking\": True,\n",
    "    \n",
    "    # Graph construction\n",
    "    \"min_entity_freq\": 2,\n",
    "    \"max_relation_distance\": 10,\n",
    "    \n",
    "    # Hybrid search\n",
    "    \"k_graph\": 5,\n",
    "    \"k_vector\": 3,\n",
    "    \"alpha\": 0.7,\n",
    "    \"search_type\": \"script\",\n",
    "    \"similarity_threshold\": None,\n",
    "    \n",
    "    # OpenSearch config\n",
    "    \"index_settings\": None,\n",
    "    \"knn_params\": None,\n",
    "    \n",
    "    # API settings\n",
    "    \"max_retries\": 5,\n",
    "    \"min_delay\": 1.0,\n",
    "    \"max_delay\": 60.0\n",
    "}\n",
    "\n",
    "print(\"Note: This notebook uses Amazon Neptune and OpenSearch which incur costs.\")\n",
    "print(\"Neptune and OpenSearch resources will be cleaned up after benchmarking.\")\n",
    "print(\"Set cleanup_enabled=False in NEPTUNE_CONFIG if you want to preserve the resources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import implementations\n",
    "implementation_path = str(project_root / 'rag_implementations/graph_rag/implementation.ipynb')\n",
    "ingestion_path = str(project_root / 'rag_implementations/graph_rag/ingestion.ipynb')\n",
    "GraphRAG = notebook_to_module(implementation_path).GraphRAG\n",
    "ingest_documents = notebook_to_module(ingestion_path).ingest_documents\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGMetricsEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and examine dataset\n",
    "print(f\"Loading {DATASET_NAME}...\")\n",
    "dataset, documents = load_labeled_dataset(DATASET_DIR, download_if_missing=True)\n",
    "print(f\"Loaded {len(dataset.examples)} examples and {len(documents)} documents\")\n",
    "\n",
    "# Get evaluation examples\n",
    "eval_examples = dataset.examples[:NUM_EVAL_SAMPLES] if NUM_EVAL_SAMPLES else dataset.examples\n",
    "print(f\"Using {len(eval_examples)} examples for evaluation\")\n",
    "\n",
    "# Examine dataset structure\n",
    "dataset_info = examine_dataset_structure(dataset, documents)\n",
    "print(\"\\nDataset Structure:\")\n",
    "print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "# Save dataset info\n",
    "save_dataset_info(dataset_info, DATASET_DIR / 'dataset_info.json')\n",
    "print(f\"\\nDataset information saved to: {DATASET_DIR / 'dataset_info.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize GraphRAG with configuration\n",
    "print(\"Initializing GraphRAG...\")\n",
    "rag = None\n",
    "try:\n",
    "    rag = GraphRAG(\n",
    "        index_name=f\"{DATASET_NAME.lower()}-benchmark\",\n",
    "        graph_store_config=NEPTUNE_CONFIG,\n",
    "        **RAG_CONFIG\n",
    "    )\n",
    "    \n",
    "    # Check if documents already exist\n",
    "    print(\"\\nChecking existing documents...\")\n",
    "    doc_count = rag.vector_store.opensearch.count(index=rag.index_name)['count']\n",
    "    if doc_count > 0:\n",
    "        print(f\"Found {doc_count} documents already indexed\")\n",
    "        print(\"Skipping ingestion to avoid duplicates\")\n",
    "    else:\n",
    "        print(\"Ingesting documents...\")\n",
    "        source_dir = DATASET_DIR / \"source_files\"\n",
    "        ingest_documents(\n",
    "            str(source_dir),\n",
    "            rag,\n",
    "            metadata={'dataset': DATASET_NAME},\n",
    "            batch_size=100\n",
    "        )\n",
    "        \n",
    "    # Print graph store status\n",
    "    print(\"\\nGraph Store Status:\")\n",
    "    if rag.graph_store and rag.graph_store._initialized:\n",
    "        print(\"✅ Neptune connection successful\")\n",
    "        print(f\"Endpoint: {rag.graph_store.graph.endpoint}\")\n",
    "    else:\n",
    "        print(\"❌ Neptune connection not initialized\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during initialization: {str(e)}\")\n",
    "    if rag:\n",
    "        # Clean up on initialization failure, but don't delete resources\n",
    "        rag.cleanup(delete_resources=False)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation\n",
    "print(\"Running evaluation...\")\n",
    "total = len(eval_examples)\n",
    "\n",
    "# Progress bar for overall evaluation\n",
    "with tqdm_notebook(total=total, desc=\"Evaluating\") as pbar:\n",
    "    # Generate answers with progress tracking\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    answers = []\n",
    "    references = []\n",
    "    graph_contexts = []\n",
    "    graph_query_times = []\n",
    "    \n",
    "    for i, example in enumerate(eval_examples):\n",
    "        try:\n",
    "            result = rag.query(example.query)\n",
    "            \n",
    "            questions.append(example.query)\n",
    "            contexts.append([doc['content'] for doc in result['context']])\n",
    "            answers.append(result['response'])\n",
    "            references.append(example.reference_answer)\n",
    "            graph_contexts.append(result['graph_context'])\n",
    "            \n",
    "            # Track graph query performance\n",
    "            if 'graph_query_time' in result:\n",
    "                graph_query_times.append(result['graph_query_time'])\n",
    "            \n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'Query': f\"{i+1}/{total}\",\n",
    "                'Status': 'Success'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            pbar.set_postfix({\n",
    "                'Query': f\"{i+1}/{total}\",\n",
    "                'Status': f'Error: {type(e).__name__}'\n",
    "            })\n",
    "            print(f\"\\nError processing query {i+1}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Create evaluation dataset\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"reference\": references,\n",
    "    \"graph_contexts\": graph_contexts\n",
    "}\n",
    "eval_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Calculate standard RAG metrics\n",
    "print(\"\\nCalculating standard RAG metrics...\")\n",
    "rag_results = evaluator.evaluate_labeled(\n",
    "    queries=questions,\n",
    "    contexts=contexts,\n",
    "    generated_answers=answers,\n",
    "    reference_answers=references,\n",
    "    plot_results=True\n",
    ")\n",
    "\n",
    "# Calculate graph-specific metrics\n",
    "print(\"\\nCalculating graph metrics...\")\n",
    "graph_metrics = calculate_graph_metrics(graph_contexts)\n",
    "\n",
    "# Add performance metrics\n",
    "if graph_query_times:\n",
    "    graph_metrics['performance'] = {\n",
    "        'avg_query_time': sum(graph_query_times) / len(graph_query_times),\n",
    "        'min_query_time': min(graph_query_times),\n",
    "        'max_query_time': max(graph_query_times)\n",
    "    }\n",
    "\n",
    "# Plot graph metrics\n",
    "plot_graph_metrics(graph_metrics)\n",
    "\n",
    "# Convert results to pandas DataFrame\n",
    "df = rag_results.to_pandas()\n",
    "\n",
    "# Add graph metrics\n",
    "for metric, value in graph_metrics.items():\n",
    "    if not isinstance(value, dict):\n",
    "        df[f\"graph_{metric}\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save results\n",
    "results_dir = project_root / \"evaluation_pipelines/rag_evaluations/results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_data = {\n",
    "    'dataset': DATASET_NAME,\n",
    "    'num_examples': len(dataset.examples),\n",
    "    'num_documents': len(documents),\n",
    "    'num_evaluated': len(eval_examples),\n",
    "    'rag_config': RAG_CONFIG,\n",
    "    'neptune_config': NEPTUNE_CONFIG,\n",
    "    'metrics': df.to_dict(),\n",
    "    'graph_metrics': graph_metrics,\n",
    "    'evaluation_data': {\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'references': references\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = results_dir / f'graph_rag_results_{DATASET_NAME.lower()}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example queries\n",
    "example_queries = [\n",
    "    \"What is the main focus of the article 'The Origin of COVID-19 and Why It Matters'?\",\n",
    "    \"What evidence suggests that SARS-CoV-2 emerged naturally rather than being engineered?\",\n",
    "    \"What are some potential consequences of not understanding how COVID-19 emerged?\"\n",
    "]\n",
    "\n",
    "print(\"Testing example queries...\\n\")\n",
    "for query in example_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    result = rag.query(query)\n",
    "    \n",
    "    print(\"\\nGraph Context:\")\n",
    "    for ctx in result['graph_context']:\n",
    "        print(f\"\\nDocument {ctx['doc_id']}:\")\n",
    "        print(\"Entities:\", \", \".join([f\"{e['text']} ({e['label']})\" for e in ctx['entities']]))\n",
    "        print(\"Relations:\", \", \".join([f\"{r['from']} {r['label']} {r['to']}\" for r in ctx['relations']]))\n",
    "    \n",
    "    print(f\"\\nResponse: {result['response']}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Resource Cleanup\n",
    "if NEPTUNE_CONFIG['cleanup_enabled']:\n",
    "    print(\"=== Cleaning Up Resources ===\")\n",
    "    print(\"Warning: This will delete all resources and indexed data\")\n",
    "    print(\"This may take 15-20 minutes to complete\")\n",
    "    try:\n",
    "        if rag:\n",
    "            # Delete resources and wait for deletion to complete\n",
    "            rag.cleanup(delete_resources=True)\n",
    "            \n",
    "            # Wait for OpenSearch domain deletion\n",
    "            if rag.vector_store and rag.vector_store.opensearch_manager:\n",
    "                rag.vector_store.opensearch_manager._wait_for_deletion()\n",
    "            \n",
    "            # Wait for Neptune deletion\n",
    "            if rag.graph_store and rag.graph_store.neptune_manager:\n",
    "                rag.graph_store.neptune_manager.cleanup()\n",
    "                \n",
    "            print(\"✅ Cleanup successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during cleanup: {str(e)}\")\n",
    "        print(\"Some resources may need to be cleaned up manually\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
