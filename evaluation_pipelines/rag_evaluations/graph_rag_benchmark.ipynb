{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Benchmarking\n",
    "\n",
    "This notebook benchmarks our GraphRAG implementation using the Origin of Covid-19 dataset.\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Graph Construction\n",
    "- min_entity_freq: Minimum frequency for entity inclusion (default: 2)\n",
    "- max_relation_distance: Maximum token distance for relationships (default: 10)\n",
    "\n",
    "### Hybrid Search\n",
    "- k_graph: Number of graph-based results (default: 5)\n",
    "- k_vector: Number of vector-based results (default: 3)\n",
    "- alpha: Weight for combining scores (default: 0.7)\n",
    "- search_type: Type of vector search ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score (default: None)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum retry attempts (default: 5)\n",
    "- min_delay: Minimum retry delay in seconds (default: 1)\n",
    "- max_delay: Maximum retry delay in seconds (default: 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    save_dataset_info,\n",
    "    convert_to_ragas_dataset\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module\n",
    "from utils.aws.opensearch_utils import OpenSearchManager\n",
    "\n",
    "# Import graph RAG components\n",
    "from rag_implementations.graph_rag.components.benchmark import run_evaluation, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "DATASET_NAME = \"OriginOfCovid19Dataset\"\n",
    "DATASET_DIR = project_root / \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "NUM_EVAL_SAMPLES = None  # Set to a number for partial evaluation\n",
    "\n",
    "# Neptune Configuration\n",
    "NEPTUNE_CONFIG = {\n",
    "    \"cluster_name\": \"test-graph-rag-benchmark\"\n",
    "}\n",
    "\n",
    "# OpenSearch Configuration\n",
    "OPENSEARCH_CONFIG = {\n",
    "    \"domain_name\": \"graph-rag-benchmark-store\",\n",
    "    \"cleanup_enabled\": True,  # Clean up domain after benchmarking\n",
    "    \"verbose\": True          # Show detailed progress\n",
    "}\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_CONFIG = {\n",
    "    # Document processing\n",
    "    \"chunk_size\": 500,  # 500 words ≈ 2000 chars\n",
    "    \"chunk_overlap\": 50,  # 50 words overlap\n",
    "    \"enable_chunking\": True,\n",
    "    \n",
    "    # Graph construction\n",
    "    \"min_entity_freq\": 2,\n",
    "    \"max_relation_distance\": 10,\n",
    "    \n",
    "    # Hybrid search\n",
    "    \"k_graph\": 5,\n",
    "    \"k_vector\": 3,\n",
    "    \"alpha\": 0.7,\n",
    "    \"search_type\": \"script\",\n",
    "    \"similarity_threshold\": None,\n",
    "    \n",
    "    # OpenSearch config\n",
    "    \"index_settings\": None,\n",
    "    \n",
    "    # API settings\n",
    "    \"max_retries\": 5,\n",
    "    \"min_delay\": 1.0,\n",
    "    \"max_delay\": 60.0\n",
    "}\n",
    "\n",
    "print(\"Note: This notebook uses Amazon Neptune and OpenSearch which incur costs.\")\n",
    "print(\"Neptune and OpenSearch resources will be cleaned up after benchmarking.\")\n",
    "print(\"Set cleanup_enabled = False if you want to preserve the resources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import implementations\n",
    "implementation_path = str(project_root / 'rag_implementations/graph_rag/implementation.ipynb')\n",
    "ingestion_path = str(project_root / 'rag_implementations/graph_rag/ingestion.ipynb')\n",
    "GraphRAG = notebook_to_module(implementation_path).GraphRAG\n",
    "ingest_documents = notebook_to_module(ingestion_path).ingest_documents\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGMetricsEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine dataset\n",
    "print(f\"Loading {DATASET_NAME}...\")\n",
    "dataset, documents = load_labeled_dataset(DATASET_DIR, download_if_missing=True)\n",
    "print(f\"Loaded {len(dataset.examples)} examples and {len(documents)} documents\")\n",
    "\n",
    "# Get evaluation examples\n",
    "eval_examples = dataset.examples[:NUM_EVAL_SAMPLES] if NUM_EVAL_SAMPLES else dataset.examples\n",
    "print(f\"Using {len(eval_examples)} examples for evaluation\")\n",
    "\n",
    "# Examine dataset structure\n",
    "dataset_info = examine_dataset_structure(dataset, documents)\n",
    "print(\"\\nDataset Structure:\")\n",
    "print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "# Save dataset info\n",
    "save_dataset_info(dataset_info, DATASET_DIR / 'dataset_info.json')\n",
    "print(f\"\\nDataset information saved to: {DATASET_DIR / 'dataset_info.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenSearch domain and get endpoint\n",
    "print(\"Setting up OpenSearch domain...\")\n",
    "opensearch_manager = OpenSearchManager(**OPENSEARCH_CONFIG)\n",
    "endpoint = opensearch_manager.setup_domain()\n",
    "\n",
    "# Set endpoint in environment for vector store\n",
    "os.environ['OPENSEARCH_HOST'] = endpoint\n",
    "\n",
    "# Initialize GraphRAG with configuration\n",
    "print(\"\\nInitializing GraphRAG...\")\n",
    "rag = None\n",
    "try:\n",
    "    rag = GraphRAG(\n",
    "        index_name=f\"{DATASET_NAME.lower()}-benchmark\",\n",
    "        graph_store_config=NEPTUNE_CONFIG,\n",
    "        **RAG_CONFIG\n",
    "    )\n",
    "    \n",
    "    # Check if documents already exist\n",
    "    print(\"\\nChecking existing documents...\")\n",
    "    doc_count = rag.vector_store.opensearch_client.client.count(index=rag.index_name)['count']\n",
    "    if doc_count > 0:\n",
    "        print(f\"Found {doc_count} documents already indexed\")\n",
    "        print(\"Skipping ingestion to avoid duplicates\")\n",
    "    else:\n",
    "        print(\"Ingesting documents...\")\n",
    "        source_dir = DATASET_DIR / \"source_files\"\n",
    "        ingest_documents(\n",
    "            str(source_dir),\n",
    "            rag,\n",
    "            metadata={'dataset': DATASET_NAME},\n",
    "            batch_size=100\n",
    "        )\n",
    "        \n",
    "    # Print graph store status\n",
    "    print(\"\\nGraph Store Status:\")\n",
    "    if rag.graph_store and rag.graph_store._initialized:\n",
    "        print(\"✅ Neptune connection successful\")\n",
    "        print(f\"Endpoint: {rag.graph_store.endpoint}\")\n",
    "    else:\n",
    "        print(\"❌ Neptune connection not initialized\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during initialization: {str(e)}\")\n",
    "    if rag:\n",
    "        # Clean up on initialization failure, but don't delete resources\n",
    "        rag.cleanup(delete_resources=False)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation and save results\n",
    "results_dir = project_root / \"evaluation_pipelines/rag_evaluations/results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results = run_evaluation(rag, dataset, evaluator, eval_examples)\n",
    "save_results(results, DATASET_NAME, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "example_queries = [\n",
    "    \"What is the main focus of the article 'The Origin of COVID-19 and Why It Matters'?\",\n",
    "    \"What evidence suggests that SARS-CoV-2 emerged naturally rather than being engineered?\",\n",
    "    \"What are some potential consequences of not understanding how COVID-19 emerged?\"\n",
    "]\n",
    "\n",
    "print(\"Testing example queries...\\n\")\n",
    "for query in example_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    result = rag.query(query)\n",
    "    \n",
    "    print(\"\\nGraph Context:\")\n",
    "    for ctx in result['graph_context']:\n",
    "        print(f\"\\nDocument {ctx['doc_id']}:\")\n",
    "        print(\"Entities:\", \", \".join([f\"{e['text']} ({e['label']})\" for e in ctx['entities']]))\n",
    "        print(\"Relations:\", \", \".join([f\"{r['from']} {r['label']} {r['to']}\" for r in ctx['relations']]))\n",
    "    \n",
    "    print(f\"\\nResponse: {result['response']}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Cleanup\n",
    "if OPENSEARCH_CONFIG['cleanup_enabled']:\n",
    "    print(\"=== Cleaning Up Resources ===\")\n",
    "    print(\"Warning: This will delete all resources and indexed data\")\n",
    "    print(\"This may take 15-20 minutes to complete\")\n",
    "    try:\n",
    "        if rag:\n",
    "            # Delete resources and wait for deletion to complete\n",
    "            rag.cleanup(delete_resources=True)\n",
    "            \n",
    "            # Wait for OpenSearch domain deletion\n",
    "            if opensearch_manager:\n",
    "                opensearch_manager._wait_for_deletion()\n",
    "            \n",
    "            # Wait for Neptune deletion\n",
    "            if rag.graph_store and rag.graph_store.neptune_manager:\n",
    "                rag.graph_store.neptune_manager.cleanup()\n",
    "                \n",
    "            print(\"✅ Cleanup successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during cleanup: {str(e)}\")\n",
    "        print(\"Some resources may need to be cleaned up manually\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
