{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Benchmarking\n",
    "\n",
    "This notebook benchmarks our GraphRAG implementation using the Origin of Covid-19 dataset for comparison with baseline RAG.\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Graph Construction\n",
    "- min_entity_freq: Minimum frequency for entity inclusion (default: 2)\n",
    "- max_relation_distance: Maximum token distance for relationships (default: 10)\n",
    "- confidence_threshold: Minimum confidence for relations (default: 0.5)\n",
    "\n",
    "### Hybrid Search\n",
    "- k_graph: Number of graph-based results (default: 5)\n",
    "- k_vector: Number of vector-based results (default: 3)\n",
    "- alpha: Weight for combining scores (default: 0.7)\n",
    "\n",
    "### Neptune Settings\n",
    "- instance_type: Neptune instance type (default: 'db.r6g.xlarge')\n",
    "- enable_audit: Enable audit logging (default: True)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum number of retry attempts (default: 5)\n",
    "- min_delay: Minimum delay between retries in seconds (default: 1)\n",
    "- max_delay: Maximum delay between retries in seconds (default: 60)\n",
    "\n",
    "## Prerequisites\n",
    "- Run setup.ipynb first to configure environment\n",
    "- Neptune cluster must be configured\n",
    "- SpaCy model must be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from datasets import Dataset\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    save_dataset_info\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset Configuration\n",
    "DATASET_NAME = \"OriginOfCovid19Dataset\"\n",
    "DATASET_DIR = project_root / \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "NUM_EVAL_SAMPLES = None  # Set to a number for partial evaluation\n",
    "\n",
    "# Neptune Configuration\n",
    "NEPTUNE_INSTANCE = \"graph-rag-benchmark-store\"\n",
    "CLEANUP_RESOURCES = True  # Default to cleaning up to avoid unexpected costs\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_CONFIG = {\n",
    "    # Document processing\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"enable_chunking\": True,\n",
    "    \n",
    "    # Graph construction\n",
    "    \"min_entity_freq\": 2,\n",
    "    \"max_relation_distance\": 10,\n",
    "    \"confidence_threshold\": 0.5,\n",
    "    \n",
    "    # Hybrid search\n",
    "    \"k_graph\": 5,\n",
    "    \"k_vector\": 3,\n",
    "    \"alpha\": 0.7,\n",
    "    \n",
    "    # Neptune settings\n",
    "    \"instance_type\": \"db.r6g.xlarge\",\n",
    "    \"enable_audit\": True,\n",
    "    \n",
    "    # API settings\n",
    "    \"max_retries\": 5,\n",
    "    \"min_delay\": 1.0,\n",
    "    \"max_delay\": 60.0\n",
    "}\n",
    "\n",
    "print(\"Note: This notebook uses Amazon Neptune which incurs costs.\")\n",
    "print(\"CLEANUP_RESOURCES is enabled by default to delete resources after benchmarking.\")\n",
    "print(\"Set CLEANUP_RESOURCES = False if you want to preserve the Neptune instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import implementations\n",
    "implementation_path = str(project_root / 'rag_implementations/graph_rag/implementation.ipynb')\n",
    "ingestion_path = str(project_root / 'rag_implementations/graph_rag/ingestion.ipynb')\n",
    "GraphRAG = notebook_to_module(implementation_path).GraphRAG\n",
    "ingest_documents = notebook_to_module(ingestion_path).ingest_documents\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGMetricsEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and examine dataset\n",
    "print(f\"Loading {DATASET_NAME}...\")\n",
    "dataset, documents = load_labeled_dataset(DATASET_DIR, download_if_missing=True)\n",
    "print(f\"Loaded {len(dataset.examples)} examples and {len(documents)} documents\")\n",
    "\n",
    "# Get evaluation examples\n",
    "eval_examples = dataset.examples[:NUM_EVAL_SAMPLES] if NUM_EVAL_SAMPLES else dataset.examples\n",
    "print(f\"Using {len(eval_examples)} examples for evaluation\")\n",
    "\n",
    "# Examine dataset structure\n",
    "dataset_info = examine_dataset_structure(dataset, documents)\n",
    "print(\"\\nDataset Structure:\")\n",
    "print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "# Save dataset info\n",
    "save_dataset_info(dataset_info, DATASET_DIR / 'dataset_info.json')\n",
    "print(f\"\\nDataset information saved to: {DATASET_DIR / 'dataset_info.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize GraphRAG with configuration\n",
    "print(\"Initializing GraphRAG...\")\n",
    "rag = GraphRAG(\n",
    "    index_name=f\"{DATASET_NAME.lower()}-benchmark\",\n",
    "    chunk_size=RAG_CONFIG[\"chunk_size\"],\n",
    "    chunk_overlap=RAG_CONFIG[\"chunk_overlap\"],\n",
    "    enable_chunking=RAG_CONFIG[\"enable_chunking\"],\n",
    "    min_entity_freq=RAG_CONFIG[\"min_entity_freq\"],\n",
    "    max_relation_distance=RAG_CONFIG[\"max_relation_distance\"],\n",
    "    confidence_threshold=RAG_CONFIG[\"confidence_threshold\"],\n",
    "    k_graph=RAG_CONFIG[\"k_graph\"],\n",
    "    k_vector=RAG_CONFIG[\"k_vector\"],\n",
    "    alpha=RAG_CONFIG[\"alpha\"],\n",
    "    instance_type=RAG_CONFIG[\"instance_type\"],\n",
    "    enable_audit=RAG_CONFIG[\"enable_audit\"],\n",
    "    max_retries=RAG_CONFIG[\"max_retries\"],\n",
    "    min_delay=RAG_CONFIG[\"min_delay\"],\n",
    "    max_delay=RAG_CONFIG[\"max_delay\"]\n",
    ")\n",
    "\n",
    "# Ingest documents\n",
    "print(\"\\nIngesting documents...\")\n",
    "source_dir = DATASET_DIR / \"source_files\"\n",
    "ingest_documents(\n",
    "    str(source_dir),\n",
    "    rag,\n",
    "    metadata={'dataset': DATASET_NAME},\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation\n",
    "def run_evaluation():\n",
    "    print(\"Running evaluation...\")\n",
    "    total = len(eval_examples)\n",
    "    \n",
    "    # Progress bar for overall evaluation\n",
    "    with tqdm_notebook(total=total, desc=\"Evaluating\") as pbar:\n",
    "        # Generate answers with progress tracking\n",
    "        questions = []\n",
    "        contexts = []\n",
    "        answers = []\n",
    "        references = []\n",
    "        graph_contexts = []\n",
    "        \n",
    "        for i, example in enumerate(eval_examples):\n",
    "            try:\n",
    "                result = rag.query(example.query)\n",
    "                \n",
    "                questions.append(example.query)\n",
    "                contexts.append([doc['content'] for doc in result['context']])\n",
    "                answers.append(result['response'])\n",
    "                references.append(example.reference_answer)\n",
    "                graph_contexts.append(result['graph_context'])\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': 'Success'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': f'Error: {type(e).__name__}'\n",
    "                })\n",
    "                raise\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"reference\": references,\n",
    "        \"graph_contexts\": graph_contexts\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    # Evaluate results\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    try:\n",
    "        # Standard RAG metrics\n",
    "        results = evaluator.evaluate_labeled(\n",
    "            queries=questions,\n",
    "            contexts=contexts,\n",
    "            generated_answers=answers,\n",
    "            reference_answers=references,\n",
    "            plot_results=True\n",
    "        )\n",
    "        \n",
    "        # Graph-specific metrics\n",
    "        graph_metrics = calculate_graph_metrics(graph_contexts)\n",
    "        \n",
    "        # Combine metrics\n",
    "        df = results.to_pandas()\n",
    "        df = pd.concat([df, pd.DataFrame([graph_metrics])], axis=1)\n",
    "        \n",
    "        # Return both raw results and DataFrame\n",
    "        return {\n",
    "            'raw_results': results,\n",
    "            'metrics_df': df.to_dict(),\n",
    "            'data': data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {type(e).__name__}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        print(\"\\nDataset contents:\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"\\n{key}:\")\n",
    "            print(f\"Type: {type(value)}\")\n",
    "            print(f\"Length: {len(value)}\")\n",
    "            print(f\"First item: {value[0][:100]}...\")\n",
    "        raise\n",
    "\n",
    "def calculate_graph_metrics(graph_contexts):\n",
    "    \"\"\"Calculate graph-specific metrics.\"\"\"\n",
    "    total_entities = 0\n",
    "    total_relations = 0\n",
    "    entity_types = {}\n",
    "    relation_types = {}\n",
    "    \n",
    "    for ctx in graph_contexts:\n",
    "        for doc_ctx in ctx:\n",
    "            # Count entities\n",
    "            doc_entities = doc_ctx['entities']\n",
    "            total_entities += len(doc_entities)\n",
    "            \n",
    "            # Track entity types\n",
    "            for entity in doc_entities:\n",
    "                entity_type = entity['label']\n",
    "                entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "            \n",
    "            # Count relations\n",
    "            doc_relations = doc_ctx['relations']\n",
    "            total_relations += len(doc_relations)\n",
    "            \n",
    "            # Track relation types\n",
    "            for relation in doc_relations:\n",
    "                relation_type = relation['label']\n",
    "                relation_types[relation_type] = relation_types.get(relation_type, 0) + 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_contexts = len(graph_contexts)\n",
    "    avg_entities = total_entities / num_contexts if num_contexts > 0 else 0\n",
    "    avg_relations = total_relations / num_contexts if num_contexts > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'avg_entities_per_context': avg_entities,\n",
    "        'avg_relations_per_context': avg_relations,\n",
    "        'unique_entity_types': len(entity_types),\n",
    "        'unique_relation_types': len(relation_types)\n",
    "    }\n",
    "\n",
    "results = run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save results\n",
    "results_dir = project_root / \"evaluation_pipelines/rag_evaluations/results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_data = {\n",
    "    'dataset': DATASET_NAME,\n",
    "    'num_examples': len(dataset.examples),\n",
    "    'num_documents': len(documents),\n",
    "    'num_evaluated': len(eval_examples),\n",
    "    'rag_config': RAG_CONFIG,\n",
    "    'metrics': results['metrics_df'],\n",
    "    'evaluation_data': {\n",
    "        'questions': results['data']['question'],\n",
    "        'answers': results['data']['answer'],\n",
    "        'references': results['data']['reference']\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = results_dir / f'graph_rag_results_{DATASET_NAME.lower()}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot graph metrics\n",
    "def plot_graph_metrics(metrics_df):\n",
    "    \"\"\"Plot graph-specific metrics.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot averages\n",
    "    plt.subplot(1, 2, 1)\n",
    "    averages = [\n",
    "        metrics_df['avg_entities_per_context'].iloc[0],\n",
    "        metrics_df['avg_relations_per_context'].iloc[0]\n",
    "    ]\n",
    "    plt.bar(['Entities', 'Relations'], averages)\n",
    "    plt.title('Average Entities and Relations per Context')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Plot unique types\n",
    "    plt.subplot(1, 2, 2)\n",
    "    unique_types = [\n",
    "        metrics_df['unique_entity_types'].iloc[0],\n",
    "        metrics_df['unique_relation_types'].iloc[0]\n",
    "    ]\n",
    "    plt.bar(['Entity Types', 'Relation Types'], unique_types)\n",
    "    plt.title('Unique Entity and Relation Types')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics\n",
    "metrics_df = pd.DataFrame(results['metrics_df'])\n",
    "plot_graph_metrics(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example queries\n",
    "example_queries = [\n",
    "    \"What is the main focus of the article 'The Origin of COVID-19 and Why It Matters'?\",\n",
    "    \"What evidence suggests that SARS-CoV-2 emerged naturally rather than being engineered?\",\n",
    "    \"What are some potential consequences of not understanding how COVID-19 emerged?\"\n",
    "]\n",
    "\n",
    "print(\"Testing example queries...\\n\")\n",
    "for query in example_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    result = rag.query(query)\n",
    "    \n",
    "    print(\"\\nGraph Context:\")\n",
    "    for ctx in result['graph_context']:\n",
    "        print(f\"\\nDocument {ctx['doc_id']}:\")\n",
    "        print(\"Entities:\", \", \".join([f\"{e['text']} ({e['label']})\" for e in ctx['entities']]))\n",
    "        print(\"Relations:\", \", \".join([f\"{r['from']} {r['label']} {r['to']}\" for r in ctx['relations']]))\n",
    "    \n",
    "    print(f\"\\nResponse: {result['response']}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Resource Cleanup\n",
    "if CLEANUP_RESOURCES:\n",
    "    print(\"Cleaning up resources...\")\n",
    "    rag.neptune_manager.cleanup()\n",
    "    rag.opensearch.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
