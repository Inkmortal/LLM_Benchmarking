{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline RAG Benchmarking\n",
    "\n",
    "This notebook benchmarks our baseline RAG implementation using the Origin of Covid-19 dataset.\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Vector Search\n",
    "- k: Number of context documents to retrieve (default: 3)\n",
    "- search_type: Type of vector search to use ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score to include (default: None)\n",
    "\n",
    "### OpenSearch\n",
    "- index_settings: Custom index settings for performance tuning\n",
    "- knn_params: Parameters for k-NN algorithm (e.g., ef_search)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum number of retry attempts (default: 5)\n",
    "- min_delay: Minimum delay between retries in seconds (default: 1)\n",
    "- max_delay: Maximum delay between retries in seconds (default: 60)\n",
    "\n",
    "## Prerequisites\n",
    "- Run setup.ipynb first to configure environment\n",
    "\n",
    "## Process\n",
    "1. Configure RAG parameters\n",
    "2. Load and examine dataset\n",
    "3. Run baseline RAG evaluation\n",
    "4. Analyze and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from ragas.evaluation import EvaluationDataset, Sample\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.aws.opensearch_utils import OpenSearchManager\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.visualization.comparison_plots import BenchmarkVisualizer\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    save_dataset_info\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "DATASET_NAME = \"OriginOfCovid19Dataset\"\n",
    "DATASET_DIR = project_root / \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "NUM_EVAL_SAMPLES = None  # Set to a number for partial evaluation\n",
    "\n",
    "# OpenSearch Configuration\n",
    "OPENSEARCH_DOMAIN = \"baseline-rag-benchmark-store\"\n",
    "CLEANUP_RESOURCES = True  # Default to cleaning up to avoid unexpected costs\n",
    "\n",
    "# RAG Configuration\n",
    "RAG_CONFIG = {\n",
    "    # Document processing\n",
    "    \"chunk_size\": 500,  # 500 words â‰ˆ 2000 chars\n",
    "    \"chunk_overlap\": 50,  # 50 words overlap\n",
    "    \"enable_chunking\": True,\n",
    "    \n",
    "    # Vector search\n",
    "    \"search_type\": \"script\",  # 'script' or 'knn'\n",
    "    \"similarity_threshold\": None,  # Minimum similarity score\n",
    "    \n",
    "    # OpenSearch settings\n",
    "    \"index_settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"knn\": {\n",
    "            \"algo_param\": {\n",
    "                \"ef_search\": 512  # Higher = more accurate but slower\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"knn_params\": {\n",
    "        \"ef_construction\": 512,  # Higher = more accurate index\n",
    "        \"m\": 16  # Higher = more connections per node\n",
    "    },\n",
    "    \n",
    "    # API settings\n",
    "    \"max_retries\": 5,\n",
    "    \"min_delay\": 1.0,\n",
    "    \"max_delay\": 60.0\n",
    "}\n",
    "\n",
    "print(\"Note: This notebook uses Amazon OpenSearch which incurs costs.\")\n",
    "print(\"CLEANUP_RESOURCES is enabled by default to delete resources after benchmarking.\")\n",
    "print(\"Set CLEANUP_RESOURCES = False if you want to preserve the OpenSearch domain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenSearch Setup\n",
    "print(\"Setting up OpenSearch...\")\n",
    "manager = OpenSearchManager(\n",
    "    domain_name=OPENSEARCH_DOMAIN,\n",
    "    cleanup_enabled=CLEANUP_RESOURCES,\n",
    "    verbose=False  # Don't print detailed status\n",
    ")\n",
    "\n",
    "# Set up domain and get endpoint\n",
    "endpoint = manager.setup_domain()\n",
    "os.environ['OPENSEARCH_HOST'] = endpoint\n",
    "\n",
    "# Import implementations\n",
    "implementation_path = str(project_root / 'rag_implementations/baseline_rag/implementation.ipynb')\n",
    "ingestion_path = str(project_root / 'rag_implementations/baseline_rag/ingestion.ipynb')\n",
    "BaselineRAG = notebook_to_module(implementation_path).BaselineRAG\n",
    "ingest_documents = notebook_to_module(ingestion_path).ingest_documents\n",
    "\n",
    "# Initialize evaluator and visualizer\n",
    "evaluator = RAGMetricsEvaluator()\n",
    "visualizer = BenchmarkVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine dataset\n",
    "print(f\"Loading {DATASET_NAME}...\")\n",
    "# Will automatically download if not found\n",
    "dataset, documents = load_labeled_dataset(DATASET_DIR, download_if_missing=True)\n",
    "print(f\"Loaded {len(dataset.examples)} examples and {len(documents)} documents\")\n",
    "\n",
    "# Examine dataset structure\n",
    "dataset_info = examine_dataset_structure(dataset, documents)\n",
    "print(\"\\nDataset Structure:\")\n",
    "print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "# Save dataset info\n",
    "save_dataset_info(dataset_info, DATASET_DIR / 'dataset_info.json')\n",
    "print(f\"\\nDataset information saved to: {DATASET_DIR / 'dataset_info.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline RAG with configuration\n",
    "print(\"Initializing baseline RAG...\")\n",
    "rag = BaselineRAG(\n",
    "    index_name=f\"{DATASET_NAME.lower()}-benchmark\",\n",
    "    chunk_size=RAG_CONFIG[\"chunk_size\"],\n",
    "    chunk_overlap=RAG_CONFIG[\"chunk_overlap\"],\n",
    "    enable_chunking=RAG_CONFIG[\"enable_chunking\"],\n",
    "    search_type=RAG_CONFIG[\"search_type\"],\n",
    "    similarity_threshold=RAG_CONFIG[\"similarity_threshold\"],\n",
    "    index_settings=RAG_CONFIG[\"index_settings\"],\n",
    "    knn_params=RAG_CONFIG[\"knn_params\"],\n",
    "    max_retries=RAG_CONFIG[\"max_retries\"],\n",
    "    min_delay=RAG_CONFIG[\"min_delay\"],\n",
    "    max_delay=RAG_CONFIG[\"max_delay\"]\n",
    ")\n",
    "\n",
    "# Check if documents already exist\n",
    "print(\"\\nChecking existing documents...\")\n",
    "try:\n",
    "    doc_count = rag.opensearch.count(index=rag.index_name)['count']\n",
    "    if doc_count > 0:\n",
    "        print(f\"Found {doc_count} documents already indexed\")\n",
    "        print(\"Skipping ingestion to avoid duplicates\")\n",
    "    else:\n",
    "        print(\"Ingesting documents...\")\n",
    "        # Use new Langchain-based ingestion\n",
    "        source_dir = DATASET_DIR / \"source_files\"\n",
    "        ingest_documents(\n",
    "            str(source_dir),\n",
    "            rag,\n",
    "            metadata={'dataset': DATASET_NAME},\n",
    "            batch_size=100\n",
    "        )\n",
    "except:\n",
    "    # Index doesn't exist yet\n",
    "    print(\"Ingesting documents...\")\n",
    "    # Use new Langchain-based ingestion\n",
    "    source_dir = DATASET_DIR / \"source_files\"\n",
    "    ingest_documents(\n",
    "        str(source_dir),\n",
    "        rag,\n",
    "        metadata={'dataset': DATASET_NAME},\n",
    "        batch_size=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "async def run_evaluation():\n",
    "    print(\"Running evaluation...\")\n",
    "    eval_examples = dataset.examples[:NUM_EVAL_SAMPLES] if NUM_EVAL_SAMPLES else dataset.examples\n",
    "    total = len(eval_examples)\n",
    "    \n",
    "    # Progress bar for overall evaluation\n",
    "    with tqdm_notebook(total=total, desc=\"Evaluating\") as pbar:\n",
    "        # Generate answers with progress tracking\n",
    "        samples = []\n",
    "        for i, example in enumerate(eval_examples):\n",
    "            try:\n",
    "                result = rag.query(example.query)\n",
    "                # Create RAGAs sample\n",
    "                sample = Sample(\n",
    "                    question=example.query,\n",
    "                    contexts=[doc.text for doc in documents],\n",
    "                    answer=result['response'],\n",
    "                    ground_truth=example.reference_answer\n",
    "                )\n",
    "                samples.append(sample)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': 'Success'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': f'Error: {type(e).__name__}'\n",
    "                })\n",
    "                raise\n",
    "\n",
    "    # Create evaluation dataset\n",
    "    print(\"\\nPreparing evaluation dataset...\")\n",
    "    eval_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "    # Evaluate results\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    try:\n",
    "        results = await evaluator.evaluate_labeled(\n",
    "            queries=[ex.query for ex in eval_examples],\n",
    "            contexts=[[doc.text] for doc in documents],\n",
    "            generated_answers=[s.answer for s in samples],\n",
    "            reference_answers=[ex.reference_answer for ex in eval_examples]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {type(e).__name__}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        print(\"\\nDataset structure:\")\n",
    "        print(f\"Number of samples: {len(samples)}\")\n",
    "        print(\"Sample structure:\")\n",
    "        print(f\"- question: {samples[0].question[:100]}...\")\n",
    "        print(f\"- contexts: {len(samples[0].contexts)} contexts\")\n",
    "        print(f\"- answer: {samples[0].answer[:100]}...\")\n",
    "        print(f\"- ground_truth: {samples[0].ground_truth[:100]}...\")\n",
    "        raise\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, score in results.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_dir = project_root / \"evaluation_pipelines/rag_evaluations/results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_data = {\n",
    "    'dataset': DATASET_NAME,\n",
    "    'num_examples': len(dataset.examples),\n",
    "    'num_documents': len(documents),\n",
    "    'num_evaluated': len(eval_examples),\n",
    "    'rag_config': RAG_CONFIG,\n",
    "    'metrics': results\n",
    "}\n",
    "\n",
    "results_file = results_dir / f'baseline_rag_results_{DATASET_NAME.lower()}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualizer.plot_comparison(\n",
    "    data={'Baseline RAG': results},\n",
    "    comparison_type=\"metrics\",\n",
    "    plot_type=\"bar\",\n",
    "    title=f'Baseline RAG Evaluation ({DATASET_NAME})'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "example_queries = [\n",
    "    \"What is the main focus of the article 'The Origin of COVID-19 and Why It Matters'?\",\n",
    "    \"What evidence suggests that SARS-CoV-2 emerged naturally rather than being engineered?\",\n",
    "    \"What are some potential consequences of not understanding how COVID-19 emerged?\"\n",
    "]\n",
    "\n",
    "print(\"Testing example queries...\\n\")\n",
    "for query in example_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    result = rag.query(query)\n",
    "    print(f\"Response: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Cleanup\n",
    "manager.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
