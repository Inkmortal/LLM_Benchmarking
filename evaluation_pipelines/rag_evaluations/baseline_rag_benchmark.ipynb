{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline RAG Benchmarking\n",
    "\n",
    "This notebook benchmarks our baseline RAG implementation using the Origin of Covid-19 dataset.\n",
    "\n",
    "## Prerequisites\n",
    "- Run setup.ipynb first to configure environment\n",
    "\n",
    "## Process\n",
    "1. Load and examine dataset\n",
    "2. Run baseline RAG evaluation\n",
    "3. Analyze and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.visualization.comparison_plots import BenchmarkVisualizer\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    prepare_documents_for_rag,\n",
    "    save_dataset_info\n",
    ")\n",
    "\n",
    "# Import RAG implementation\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "DATASET_NAME = \"OriginOfCovid19Dataset\"\n",
    "DATASET_DIR = project_root / \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "NUM_EVAL_SAMPLES = None  # Set to a number for partial evaluation\n",
    "\n",
    "# OpenSearch Configuration\n",
    "OPENSEARCH_DOMAIN = \"baseline-rag-benchmark-store\"\n",
    "CLEANUP_RESOURCES = True  # Default to cleaning up to avoid unexpected costs\n",
    "\n",
    "print(\"Note: This notebook uses Amazon OpenSearch which incurs costs.\")\n",
    "print(\"CLEANUP_RESOURCES is enabled by default to delete resources after benchmarking.\")\n",
    "print(\"Set CLEANUP_RESOURCES = False if you want to preserve the OpenSearch domain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# OpenSearch Setup\n",
    "print(\"Setting up OpenSearch...\")\n",
    "opensearch = boto3.client('opensearch')\n",
    "\n",
    "# Get current identity (works for both roles and users)\n",
    "sts = boto3.client('sts')\n",
    "identity = sts.get_caller_identity()\n",
    "current_arn = identity['Arn']\n",
    "print(f\"Using identity: {current_arn}\")\n",
    "\n",
    "try:\n",
    "    # Try to get existing domain\n",
    "    domain = opensearch.describe_domain(DomainName=OPENSEARCH_DOMAIN)\n",
    "    domain_endpoint = domain['DomainStatus']['Endpoints'].get('vpc') or domain['DomainStatus']['Endpoint']\n",
    "    print(f\"Using existing OpenSearch domain: {OPENSEARCH_DOMAIN}\")\n",
    "except:\n",
    "    print(f\"Creating new OpenSearch domain: {OPENSEARCH_DOMAIN}\")\n",
    "    # Create domain with standard config for embeddings\n",
    "    response = opensearch.create_domain(\n",
    "        DomainName=OPENSEARCH_DOMAIN,\n",
    "        EngineVersion='OpenSearch_2.11',\n",
    "        ClusterConfig={\n",
    "            'InstanceType': 't3.small.search',\n",
    "            'InstanceCount': 1,\n",
    "            'DedicatedMasterEnabled': False,\n",
    "            'ZoneAwarenessEnabled': False\n",
    "        },\n",
    "        EBSOptions={\n",
    "            'EBSEnabled': True,\n",
    "            'VolumeType': 'gp3',\n",
    "            'VolumeSize': 10\n",
    "        },\n",
    "        # Restrict access to current identity\n",
    "        AccessPolicies=json.dumps({\n",
    "            'Version': '2012-10-17',\n",
    "            'Statement': [{\n",
    "                'Effect': 'Allow',\n",
    "                'Principal': {\n",
    "                    'AWS': current_arn\n",
    "                },\n",
    "                'Action': 'es:*',\n",
    "                'Resource': f'arn:aws:es:*:*:domain/{OPENSEARCH_DOMAIN}/*'\n",
    "            }]\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    print(\"Waiting for OpenSearch domain to be active (this may take 10-15 minutes)...\")\n",
    "    max_attempts = 40\n",
    "    attempt = 0\n",
    "    \n",
    "    # Poll domain status until active\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            domain = opensearch.describe_domain(DomainName=OPENSEARCH_DOMAIN)\n",
    "            status = domain['DomainStatus']['Processing']\n",
    "            if not status:  # False means processing is complete\n",
    "                print(\"OpenSearch domain is now active\")\n",
    "                break\n",
    "            print(\"Domain still processing...\")\n",
    "        except opensearch.exceptions.ResourceNotFoundException:\n",
    "            print(\"Waiting for domain to be created...\")\n",
    "        \n",
    "        time.sleep(30)  # Wait 30 seconds before checking again\n",
    "        attempt += 1\n",
    "        \n",
    "    if attempt >= max_attempts:\n",
    "        raise TimeoutError(\"OpenSearch domain creation timed out after 20 minutes\")\n",
    "    \n",
    "    domain = opensearch.describe_domain(DomainName=OPENSEARCH_DOMAIN)\n",
    "    domain_endpoint = domain['DomainStatus']['Endpoints'].get('vpc') or domain['DomainStatus']['Endpoint']\n",
    "\n",
    "# Set environment variable\n",
    "os.environ['OPENSEARCH_HOST'] = domain_endpoint\n",
    "print(f\"OpenSearch host set to: {domain_endpoint}\")\n",
    "\n",
    "# Import BaselineRAG after setting OPENSEARCH_HOST\n",
    "implementation_path = str(project_root / 'rag_implementations/baseline_rag/implementation.ipynb')\n",
    "baseline_rag = notebook_to_module(implementation_path)\n",
    "BaselineRAG = baseline_rag.BaselineRAG\n",
    "\n",
    "# Initialize evaluator and visualizer\n",
    "evaluator = RAGMetricsEvaluator()\n",
    "visualizer = BenchmarkVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and examine dataset\n",
    "print(f\"Loading {DATASET_NAME}...\")\n",
    "# Will automatically download if not found\n",
    "dataset, documents = load_labeled_dataset(DATASET_DIR, download_if_missing=True)\n",
    "print(f\"Loaded {len(dataset.examples)} examples and {len(documents)} documents\")\n",
    "\n",
    "# Examine dataset structure\n",
    "dataset_info = examine_dataset_structure(dataset, documents)\n",
    "print(\"\\nDataset Structure:\")\n",
    "print(json.dumps(dataset_info, indent=2))\n",
    "\n",
    "# Save dataset info\n",
    "save_dataset_info(dataset_info, DATASET_DIR / 'dataset_info.json')\n",
    "print(f\"\\nDataset information saved to: {DATASET_DIR / 'dataset_info.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize baseline RAG\n",
    "print(\"Initializing baseline RAG...\")\n",
    "rag = BaselineRAG(index_name=f\"{DATASET_NAME.lower()}-benchmark\")\n",
    "\n",
    "# Prepare and ingest documents\n",
    "print(\"Preparing documents...\")\n",
    "prepared_docs = prepare_documents_for_rag(documents, DATASET_NAME)\n",
    "\n",
    "# Check if documents already exist\n",
    "print(\"Checking existing documents...\")\n",
    "try:\n",
    "    doc_count = rag.opensearch.count(index=rag.index_name)['count']\n",
    "    if doc_count > 0:\n",
    "        print(f\"Found {doc_count} documents already indexed\")\n",
    "        print(\"Skipping ingestion to avoid duplicates\")\n",
    "    else:\n",
    "        print(\"Ingesting documents...\")\n",
    "        rag.ingest_documents(prepared_docs)\n",
    "except:\n",
    "    # Index doesn't exist yet\n",
    "    print(\"Ingesting documents...\")\n",
    "    rag.ingest_documents(prepared_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run evaluation\n",
    "async def run_evaluation():\n",
    "    print(\"Running evaluation...\")\n",
    "    eval_examples = dataset.examples[:NUM_EVAL_SAMPLES] if NUM_EVAL_SAMPLES else dataset.examples\n",
    "\n",
    "    results = await evaluator.evaluate_labeled(\n",
    "        queries=[ex.query for ex in eval_examples],\n",
    "        contexts=[[doc.text] for doc in documents],\n",
    "        generated_answers=[rag.query(ex.query)['response'] for ex in eval_examples],\n",
    "        reference_answers=[ex.reference_answer for ex in eval_examples]\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, score in results.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save results\n",
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results_data = {\n",
    "    'dataset': DATASET_NAME,\n",
    "    'num_examples': len(dataset.examples),\n",
    "    'num_documents': len(documents),\n",
    "    'num_evaluated': len(eval_examples),\n",
    "    'metrics': results\n",
    "}\n",
    "\n",
    "with open(results_dir / 'baseline_rag_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_dir / 'baseline_rag_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "visualizer.plot_comparison(\n",
    "    data={'Baseline RAG': results},\n",
    "    comparison_type=\"metrics\",\n",
    "    plot_type=\"bar\",\n",
    "    title=f'Baseline RAG Evaluation ({DATASET_NAME})'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example queries\n",
    "example_queries = [\n",
    "    \"What is the main focus of the article 'The Origin of COVID-19 and Why It Matters'?\",\n",
    "    \"What evidence suggests that SARS-CoV-2 emerged naturally rather than being engineered?\",\n",
    "    \"What are some potential consequences of not understanding how COVID-19 emerged?\"\n",
    "]\n",
    "\n",
    "print(\"Testing example queries...\\n\")\n",
    "for query in example_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    result = rag.query(query)\n",
    "    print(f\"Response: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Resource Cleanup (Optional)\n",
    "if CLEANUP_RESOURCES:\n",
    "    print(\"\\n=== Cleaning Up Resources ===\")\n",
    "    print(\"Warning: This will delete the OpenSearch domain and all indexed data\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Deleting OpenSearch domain: {OPENSEARCH_DOMAIN}\")\n",
    "        opensearch.delete_domain(DomainName=OPENSEARCH_DOMAIN)\n",
    "        print(\"✅ Cleanup successful\")\n",
    "        print(\"Note: Domain deletion may take 15-20 minutes to complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during cleanup: {str(e)}\")\n",
    "else:\n",
    "    print(\"\\n=== Resource Cleanup ===\")\n",
    "    print(\"To avoid ongoing costs, you can:\")\n",
    "    print(f\"1. Set CLEANUP_RESOURCES = True and rerun this section\")\n",
    "    print(f\"2. Manually delete the OpenSearch domain '{OPENSEARCH_DOMAIN}' in AWS Console\")\n",
    "    print(\"   AWS Console > OpenSearch > Domains > Select domain > Delete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
