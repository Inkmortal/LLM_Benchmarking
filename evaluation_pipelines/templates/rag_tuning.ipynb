{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation Tuning Template\n",
    "\n",
    "This notebook provides a template for tuning a RAG implementation's parameters and configurations. It can be used to optimize:\n",
    "- Chunking parameters\n",
    "- Retrieval settings\n",
    "- Model parameters\n",
    "- Embedding configurations\n",
    "- Any other implementation-specific parameters\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Vector Search\n",
    "- k: Number of context documents to retrieve (default: 3)\n",
    "- search_type: Type of vector search to use ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score to include (default: None)\n",
    "\n",
    "### OpenSearch\n",
    "- index_settings: Custom index settings for performance tuning\n",
    "- knn_params: Parameters for k-NN algorithm (e.g., ef_search)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum number of retry attempts (default: 5)\n",
    "- min_delay: Minimum delay between retries in seconds (default: 1)\n",
    "- max_delay: Maximum delay between retries in seconds (default: 60)\n",
    "\n",
    "## Prerequisites\n",
    "- Run setup.ipynb first to configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.aws.opensearch_utils import OpenSearchManager\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    save_dataset_info\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementation Configuration\n",
    "IMPLEMENTATION = {\n",
    "    \"name\": \"baseline_rag\",\n",
    "    \"notebook_path\": \"../../rag_implementations/baseline_rag/implementation.ipynb\",\n",
    "    \"parameter_grid\": {\n",
    "        # Document processing\n",
    "        \"chunk_size\": [256, 512, 1024],  # Words per chunk\n",
    "        \"chunk_overlap\": [0, 50, 100],  # Words overlap\n",
    "        \"enable_chunking\": [True],\n",
    "        \n",
    "        # Vector search\n",
    "        \"search_type\": [\"script\", \"knn\"],\n",
    "        \"similarity_threshold\": [None, 0.7, 0.8],\n",
    "        \n",
    "        # OpenSearch settings\n",
    "        \"index_settings\": [{\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"knn\": {\n",
    "                \"algo_param\": {\n",
    "                    \"ef_search\": ef  # Higher = more accurate but slower\n",
    "                }\n",
    "            }\n",
    "        } for ef in [256, 512]],\n",
    "        \n",
    "        \"knn_params\": [{\n",
    "            \"ef_construction\": ef,  # Higher = more accurate index\n",
    "            \"m\": m  # Higher = more connections per node\n",
    "        } for ef, m in [(256, 16), (512, 16), (512, 32)]],\n",
    "        \n",
    "        # API settings\n",
    "        \"max_retries\": [3, 5],\n",
    "        \"min_delay\": [1.0],\n",
    "        \"max_delay\": [60.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASETS = {\n",
    "    \"tuning\": {\n",
    "        \"covid19\": {\n",
    "            \"name\": \"OriginOfCovid19Dataset\",\n",
    "            \"dir\": \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "        }\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"paul_graham\": {\n",
    "            \"name\": \"PaulGrahamEssaysDataset\",\n",
    "            \"dir\": \"datasets/rag_evaluation/labeled/paul_graham_essays\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# OpenSearch Configuration\n",
    "OPENSEARCH_CONFIG = {\n",
    "    \"domain_prefix\": \"rag-tuning\",\n",
    "    \"cleanup_resources\": True  # Default to cleaning up to avoid unexpected costs\n",
    "}\n",
    "\n",
    "# Metric Weights for Optimization\n",
    "METRIC_WEIGHTS = {\n",
    "    \"faithfulness\": 0.3,\n",
    "    \"context_precision\": 0.2,\n",
    "    \"response_relevancy\": 0.3,\n",
    "    \"context_recall\": 0.1,\n",
    "    \"context_entities_recall\": 0.1\n",
    "}\n",
    "\n",
    "print(\"Note: This notebook uses Amazon OpenSearch which incurs costs.\")\n",
    "print(\"CLEANUP_RESOURCES is enabled by default to delete resources after benchmarking.\")\n",
    "print(\"Set CLEANUP_RESOURCES = False if you want to preserve the OpenSearch domain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_implementation(params: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Load a RAG implementation with specific parameters.\"\"\"\n",
    "    try:\n",
    "        module = notebook_to_module(IMPLEMENTATION[\"notebook_path\"])\n",
    "        implementation = module.RAGImplementation(**params)\n",
    "        return implementation\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading implementation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_opensearch(trial_id: str) -> OpenSearchManager:\n",
    "    \"\"\"Set up OpenSearch for a tuning trial.\"\"\"\n",
    "    domain_name = f\"{OPENSEARCH_CONFIG['domain_prefix']}-{trial_id}\"\n",
    "    \n",
    "    print(f\"Setting up OpenSearch for trial {trial_id}...\")\n",
    "    manager = OpenSearchManager(\n",
    "        domain_name=domain_name,\n",
    "        cleanup_enabled=OPENSEARCH_CONFIG[\"cleanup_resources\"],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Set up domain and get endpoint\n",
    "    endpoint = manager.setup_domain()\n",
    "    os.environ['OPENSEARCH_HOST'] = endpoint\n",
    "    \n",
    "    return manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_parameter_combinations(parameter_grid: Dict[str, List]) -> List[Dict]:\n",
    "    \"\"\"Generate all possible parameter combinations from the grid.\"\"\"\n",
    "    keys = parameter_grid.keys()\n",
    "    values = parameter_grid.values()\n",
    "    combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    return combinations\n",
    "\n",
    "def calculate_aggregate_score(results: Dict[str, float], weights: Dict[str, float]) -> float:\n",
    "    \"\"\"Calculate weighted average of metrics.\"\"\"\n",
    "    score = 0\n",
    "    for metric, value in results.items():\n",
    "        if metric in weights:\n",
    "            score += value * weights[metric]\n",
    "    return score\n",
    "\n",
    "def run_evaluation(implementation: Any, dataset: Dataset, evaluator: RAGMetricsEvaluator):\n",
    "    \"\"\"Run evaluation for a parameter combination.\"\"\"\n",
    "    print(f\"Running evaluation...\")\n",
    "    total = len(dataset.examples)\n",
    "    \n",
    "    # Progress bar for overall evaluation\n",
    "    with tqdm_notebook(total=total, desc=\"Evaluating\") as pbar:\n",
    "        # Generate answers with progress tracking\n",
    "        questions = []\n",
    "        contexts = []\n",
    "        answers = []\n",
    "        references = []\n",
    "        \n",
    "        for i, example in enumerate(dataset.examples):\n",
    "            try:\n",
    "                result = implementation.query(example.query)\n",
    "                \n",
    "                questions.append(example.query)\n",
    "                contexts.append([doc['content'] for doc in result['context']])\n",
    "                answers.append(result['response'])\n",
    "                references.append(example.reference_answer)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': 'Success'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': f'Error: {type(e).__name__}'\n",
    "                })\n",
    "                raise\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"reference\": references\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    # Evaluate results\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    try:\n",
    "        results = evaluator.evaluate_labeled(\n",
    "            queries=questions,\n",
    "            contexts=contexts,\n",
    "            generated_answers=answers,\n",
    "            reference_answers=references,\n",
    "            plot_results=True\n",
    "        )\n",
    "        \n",
    "        # Convert results to pandas DataFrame\n",
    "        df = results.to_pandas()\n",
    "        \n",
    "        # Return both raw results and DataFrame\n",
    "        return {\n",
    "            'raw_results': results,\n",
    "            'metrics_df': df.to_dict(),\n",
    "            'data': data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {type(e).__name__}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        print(\"\\nDataset contents:\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"\\n{key}:\")\n",
    "            print(f\"Type: {type(value)}\")\n",
    "            print(f\"Length: {len(value)}\")\n",
    "            print(f\"First item: {value[0][:100]}...\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize evaluator\n",
    "evaluator = RAGMetricsEvaluator()\n",
    "\n",
    "# Generate parameter combinations\n",
    "parameter_combinations = generate_parameter_combinations(IMPLEMENTATION[\"parameter_grid\"])\n",
    "print(f\"Generated {len(parameter_combinations)} parameter combinations to evaluate\")\n",
    "\n",
    "# Store results for each combination\n",
    "all_results = []\n",
    "\n",
    "# Run evaluation for each combination\n",
    "for trial_id, params in enumerate(parameter_combinations):\n",
    "    print(f\"\\nEvaluating combination {trial_id + 1}/{len(parameter_combinations)}\")\n",
    "    print(\"Parameters:\", json.dumps(params, indent=2))\n",
    "    \n",
    "    # Set up OpenSearch\n",
    "    manager = setup_opensearch(f\"trial-{trial_id}\")\n",
    "    \n",
    "    # Initialize implementation with parameters\n",
    "    implementation = load_implementation(params)\n",
    "    \n",
    "    # Store results for this combination\n",
    "    trial_results = {\n",
    "        'trial_id': trial_id,\n",
    "        'parameters': params,\n",
    "        'results': {}\n",
    "    }\n",
    "    \n",
    "    # Evaluate on tuning dataset\n",
    "    for dataset_id, dataset_config in DATASETS[\"tuning\"].items():\n",
    "        print(f\"\\nEvaluating on {dataset_config['name']}...\")\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset_dir = project_root / dataset_config[\"dir\"]\n",
    "        dataset, documents = load_labeled_dataset(dataset_dir)\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = run_evaluation(implementation, dataset, evaluator)\n",
    "        trial_results['results'][dataset_id] = results\n",
    "        \n",
    "        # Calculate aggregate score\n",
    "        metrics = {k: v.mean() for k, v in results['metrics_df'].items()}\n",
    "        aggregate_score = calculate_aggregate_score(metrics, METRIC_WEIGHTS)\n",
    "        trial_results['aggregate_score'] = aggregate_score\n",
    "    \n",
    "    # Store results\n",
    "    all_results.append(trial_results)\n",
    "    \n",
    "    # Cleanup OpenSearch\n",
    "    manager.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Save tuning results.\"\"\"\n",
    "    results_dir = project_root / \"evaluation_pipelines/rag_evaluations/results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare results data\n",
    "    results_data = {\n",
    "        'implementation': IMPLEMENTATION,\n",
    "        'datasets': DATASETS,\n",
    "        'metric_weights': METRIC_WEIGHTS,\n",
    "        'trials': results\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = results_dir / f'rag_tuning_results_{timestamp}.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Save all results\n",
    "save_results(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame([\n",
    "    {**trial['parameters'], 'score': trial['aggregate_score']}\n",
    "    for trial in all_results\n",
    "])\n",
    "\n",
    "# Find best parameters\n",
    "best_trial = max(all_results, key=lambda x: x['aggregate_score'])\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(json.dumps(best_trial['parameters'], indent=2))\n",
    "print(f\"\\nAggregate Score: {best_trial['aggregate_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Validating best parameters...\")\n",
    "\n",
    "# Set up OpenSearch for validation\n",
    "manager = setup_opensearch(\"validation\")\n",
    "\n",
    "# Initialize implementation with best parameters\n",
    "implementation = load_implementation(best_trial['parameters'])\n",
    "\n",
    "# Evaluate on validation datasets\n",
    "validation_results = {}\n",
    "for dataset_id, dataset_config in DATASETS[\"validation\"].items():\n",
    "    print(f\"\\nValidating on {dataset_config['name']}...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset_dir = project_root / dataset_config[\"dir\"]\n",
    "    dataset, documents = load_labeled_dataset(dataset_dir)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = run_evaluation(implementation, dataset, evaluator)\n",
    "    validation_results[dataset_id] = results\n",
    "    \n",
    "    # Calculate aggregate score\n",
    "    metrics = {k: v.mean() for k, v in results['metrics_df'].items()}\n",
    "    aggregate_score = calculate_aggregate_score(metrics, METRIC_WEIGHTS)\n",
    "    \n",
    "    print(f\"\\nValidation Results for {dataset_config['name']}:\")\n",
    "    print(\"Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print(f\"Aggregate Score: {aggregate_score:.4f}\")\n",
    "\n",
    "# Cleanup OpenSearch\n",
    "manager.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "Use this section to analyze the parameter tuning results:\n",
    "\n",
    "1. Parameter Impact Analysis\n",
    "   - How each parameter affects performance\n",
    "   - Parameter interactions and dependencies\n",
    "   - Sensitivity to parameter changes\n",
    "\n",
    "2. Validation Performance\n",
    "   - Generalization to validation dataset\n",
    "   - Stability of performance\n",
    "   - Potential overfitting concerns\n",
    "\n",
    "3. Trade-offs\n",
    "   - Performance vs. resource usage\n",
    "   - Quality vs. speed\n",
    "   - Complexity vs. maintainability\n",
    "\n",
    "4. Recommendations\n",
    "   - Optimal parameter settings\n",
    "   - Parameter ranges to avoid\n",
    "   - Future tuning suggestions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
