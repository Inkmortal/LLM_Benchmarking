{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation Tuning Template\n",
    "\n",
    "This notebook provides a template for tuning a RAG implementation's parameters and configurations. It can be used to optimize:\n",
    "- Chunking parameters\n",
    "- Retrieval settings\n",
    "- Model parameters\n",
    "- Embedding configurations\n",
    "- Any other implementation-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "from itertools import product\n",
    "\n",
    "# Import utilities\n",
    "import utils_setup\n",
    "from utils import RAGMetricsEvaluator, BenchmarkVisualizer, notebook_to_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the implementation to tune and the parameter space to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning configuration\n",
    "config = {\n",
    "    \"implementation\": {\n",
    "        \"name\": \"baseline_rag\",  # or \"graph_rag\", etc.\n",
    "        \"notebook_path\": \"../../rag_implementations/baseline_rag/implementation.ipynb\"\n",
    "    },\n",
    "    \"parameter_grid\": {\n",
    "        \"chunk_size\": [256, 512, 1024],\n",
    "        \"chunk_overlap\": [0, 50, 100],\n",
    "        \"retrieval_k\": [3, 5, 7],\n",
    "        \"similarity_threshold\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"labeled\": [\"tuning_dataset\"],  # Dataset for parameter tuning\n",
    "        \"validation\": [\"validation_dataset\"]  # Dataset for validating tuned parameters\n",
    "    },\n",
    "    \"metrics\": [\n",
    "        \"faithfulness\",\n",
    "        \"context_precision\",\n",
    "        \"response_relevancy\",\n",
    "        \"context_recall\",\n",
    "        \"context_entities_recall\"\n",
    "    ],\n",
    "    \"optimization\": {\n",
    "        \"metric_weights\": {  # Weights for combining metrics into a single score\n",
    "            \"faithfulness\": 0.3,\n",
    "            \"context_precision\": 0.2,\n",
    "            \"response_relevancy\": 0.3,\n",
    "            \"context_recall\": 0.1,\n",
    "            \"context_entities_recall\": 0.1\n",
    "        }\n",
    "    },\n",
    "    \"batch_size\": 20,\n",
    "    \"sleep_time\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the implementation to tune\n",
    "try:\n",
    "    module = notebook_to_module(config[\"implementation\"][\"notebook_path\"])\n",
    "    RAGImplementation = module.RAGImplementation\n",
    "except Exception as e:\n",
    "    print(f\"Error loading implementation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parameter_combinations(parameter_grid: Dict[str, List]) -> List[Dict]:\n",
    "    \"\"\"Generate all possible parameter combinations from the grid.\"\"\"\n",
    "    keys = parameter_grid.keys()\n",
    "    values = parameter_grid.values()\n",
    "    combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    return combinations\n",
    "\n",
    "def calculate_aggregate_score(results: Dict[str, float], weights: Dict[str, float]) -> float:\n",
    "    \"\"\"Calculate weighted average of metrics.\"\"\"\n",
    "    score = 0\n",
    "    for metric, value in results.items():\n",
    "        if metric in weights:\n",
    "            score += value * weights[metric]\n",
    "    return score\n",
    "\n",
    "async def evaluate_parameters(\n",
    "    params: Dict,\n",
    "    dataset_name: str,\n",
    "    evaluator: RAGMetricsEvaluator\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a specific parameter combination.\"\"\"\n",
    "    # Initialize implementation with parameters\n",
    "    implementation = RAGImplementation(**params)\n",
    "    \n",
    "    # Load dataset\n",
    "    queries, contexts, reference_answers = load_llama_dataset(dataset_name)\n",
    "    \n",
    "    # Generate answers\n",
    "    generated_answers = [implementation.query(q) for q in queries]\n",
    "    \n",
    "    # Evaluate\n",
    "    results = await evaluator.evaluate_labeled(\n",
    "        queries=queries,\n",
    "        contexts=contexts,\n",
    "        generated_answers=generated_answers,\n",
    "        reference_answers=reference_answers\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_tuning_results(\n",
    "    results: List[Dict],\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"Create visualizations for parameter tuning results.\"\"\"\n",
    "    visualizer = BenchmarkVisualizer()\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create parameter impact visualizations\n",
    "    for param in config[\"parameter_grid\"].keys():\n",
    "        param_data = df.groupby(param)[\"aggregate_score\"].mean()\n",
    "        visualizer.plot_comparison(\n",
    "            param_data,\n",
    "            comparison_type=f\"Parameter Impact: {param}\",\n",
    "            plot_type=\"line\",\n",
    "            save_path=f\"{output_dir}/{param}_impact.png\"\n",
    "        )\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    visualizer.plot_comparison(\n",
    "        df[list(config[\"parameter_grid\"].keys()) + [\"aggregate_score\"]].corr(),\n",
    "        comparison_type=\"Parameter Correlations\",\n",
    "        plot_type=\"heatmap\",\n",
    "        save_path=f\"{output_dir}/parameter_correlations.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = RAGMetricsEvaluator(\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    sleep_time=config[\"sleep_time\"]\n",
    ")\n",
    "\n",
    "# Generate parameter combinations\n",
    "parameter_combinations = generate_parameter_combinations(config[\"parameter_grid\"])\n",
    "\n",
    "# Evaluate each combination\n",
    "results = []\n",
    "for params in parameter_combinations:\n",
    "    # Evaluate on tuning dataset\n",
    "    metrics = await evaluate_parameters(\n",
    "        params,\n",
    "        config[\"datasets\"][\"labeled\"][0],\n",
    "        evaluator\n",
    "    )\n",
    "    \n",
    "    # Calculate aggregate score\n",
    "    aggregate_score = calculate_aggregate_score(\n",
    "        metrics,\n",
    "        config[\"optimization\"][\"metric_weights\"]\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        **params,\n",
    "        **metrics,\n",
    "        \"aggregate_score\": aggregate_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find best parameters\n",
    "best_params = results_df.loc[results_df[\"aggregate_score\"].idxmax()]\n",
    "print(\"Best Parameters:\")\n",
    "for param in config[\"parameter_grid\"].keys():\n",
    "    print(f\"{param}: {best_params[param]}\")\n",
    "print(f\"Aggregate Score: {best_params['aggregate_score']}\")\n",
    "\n",
    "# Visualize results\n",
    "visualize_tuning_results(results, \"results/parameter_tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize implementation with best parameters\n",
    "best_implementation = RAGImplementation(**{\n",
    "    param: best_params[param]\n",
    "    for param in config[\"parameter_grid\"].keys()\n",
    "})\n",
    "\n",
    "# Evaluate on validation dataset\n",
    "validation_results = await evaluate_parameters(\n",
    "    best_params,\n",
    "    config[\"datasets\"][\"validation\"][0],\n",
    "    evaluator\n",
    ")\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for metric, value in validation_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "Use this section to analyze the parameter tuning results:\n",
    "\n",
    "1. Parameter Impact Analysis\n",
    "   - How each parameter affects performance\n",
    "   - Parameter interactions and dependencies\n",
    "   - Sensitivity to parameter changes\n",
    "\n",
    "2. Validation Performance\n",
    "   - Generalization to validation dataset\n",
    "   - Stability of performance\n",
    "   - Potential overfitting concerns\n",
    "\n",
    "3. Trade-offs\n",
    "   - Performance vs. resource usage\n",
    "   - Quality vs. speed\n",
    "   - Complexity vs. maintainability\n",
    "\n",
    "4. Recommendations\n",
    "   - Optimal parameter settings\n",
    "   - Parameter ranges to avoid\n",
    "   - Future tuning suggestions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
