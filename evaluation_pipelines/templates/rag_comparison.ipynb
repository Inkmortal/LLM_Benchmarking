{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation Comparison Template\n",
    "\n",
    "This notebook provides a template for comparing different RAG implementations. It can be used to evaluate and compare any RAG techniques, including but not limited to:\n",
    "- Baseline RAG vs GraphRAG\n",
    "- Different vector stores\n",
    "- Different chunking strategies\n",
    "- Different embedding models\n",
    "- Different retrieval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Import utilities\n",
    "import utils_setup\n",
    "from utils import RAGMetricsEvaluator, BenchmarkVisualizer, notebook_to_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the implementations to compare and evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation configuration\n",
    "config = {\n",
    "    \"implementations\": {\n",
    "        \"implementation_1\": {\n",
    "            \"name\": \"baseline_rag\",\n",
    "            \"notebook_path\": \"../../rag_implementations/baseline_rag/implementation.ipynb\"\n",
    "        },\n",
    "        \"implementation_2\": {\n",
    "            \"name\": \"graph_rag\",\n",
    "            \"notebook_path\": \"../../rag_implementations/graph_rag/implementation.ipynb\"\n",
    "        }\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"labeled\": [\"dataset1\", \"dataset2\"],  # List of labeled datasets to use\n",
    "        \"unlabeled\": [\"dataset3\"]  # List of unlabeled datasets to use\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"labeled\": [\n",
    "            \"faithfulness\",\n",
    "            \"context_precision\",\n",
    "            \"response_relevancy\",\n",
    "            \"context_recall\",\n",
    "            \"context_entities_recall\"\n",
    "        ],\n",
    "        \"unlabeled\": [\n",
    "            \"faithfulness\",\n",
    "            \"context_precision\",\n",
    "            \"response_relevancy\",\n",
    "            \"noise_sensitivity\"\n",
    "        ]\n",
    "    },\n",
    "    \"batch_size\": 20,\n",
    "    \"sleep_time\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Implementations\n",
    "\n",
    "Import the RAG implementations to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load implementations\n",
    "implementations = {}\n",
    "for impl_id, impl_config in config[\"implementations\"].items():\n",
    "    try:\n",
    "        module = notebook_to_module(impl_config[\"notebook_path\"])\n",
    "        implementations[impl_id] = module.RAGImplementation()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {impl_config['name']}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_on_dataset(dataset_name: str, implementations: Dict, is_labeled: bool = True):\n",
    "    \"\"\"Evaluate all implementations on a specific dataset.\"\"\"\n",
    "    # Initialize evaluator\n",
    "    evaluator = RAGMetricsEvaluator(\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        sleep_time=config[\"sleep_time\"]\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Load dataset\n",
    "    if is_labeled:\n",
    "        queries, contexts, reference_answers = load_llama_dataset(dataset_name)\n",
    "    else:\n",
    "        queries, contexts = load_unlabeled_dataset(dataset_name)\n",
    "    \n",
    "    # Evaluate each implementation\n",
    "    for impl_id, implementation in implementations.items():\n",
    "        # Generate answers\n",
    "        generated_answers = [implementation.query(q) for q in queries]\n",
    "        \n",
    "        # Evaluate\n",
    "        if is_labeled:\n",
    "            results[impl_id] = await evaluator.evaluate_labeled(\n",
    "                queries=queries,\n",
    "                contexts=contexts,\n",
    "                generated_answers=generated_answers,\n",
    "                reference_answers=reference_answers\n",
    "            )\n",
    "        else:\n",
    "            results[impl_id] = await evaluator.evaluate_unlabeled(\n",
    "                queries=queries,\n",
    "                contexts=contexts,\n",
    "                generated_answers=generated_answers\n",
    "            )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results: Dict[str, Any], output_dir: str):\n",
    "    \"\"\"Create visualizations for evaluation results.\"\"\"\n",
    "    visualizer = BenchmarkVisualizer()\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    visualizer.create_comparison_report(results, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on labeled datasets\n",
    "labeled_results = {}\n",
    "for dataset in config[\"datasets\"][\"labeled\"]:\n",
    "    labeled_results[dataset] = await evaluate_on_dataset(\n",
    "        dataset,\n",
    "        implementations,\n",
    "        is_labeled=True\n",
    "    )\n",
    "\n",
    "# Evaluate on unlabeled datasets\n",
    "unlabeled_results = {}\n",
    "for dataset in config[\"datasets\"][\"unlabeled\"]:\n",
    "    unlabeled_results[dataset] = await evaluate_on_dataset(\n",
    "        dataset,\n",
    "        implementations,\n",
    "        is_labeled=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = Path(\"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Visualize labeled dataset results\n",
    "visualize_results(\n",
    "    labeled_results,\n",
    "    output_dir=str(results_dir / \"labeled\")\n",
    ")\n",
    "\n",
    "# Visualize unlabeled dataset results\n",
    "visualize_results(\n",
    "    unlabeled_results,\n",
    "    output_dir=str(results_dir / \"unlabeled\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "Use this section to analyze the results and document insights about the comparison:\n",
    "\n",
    "1. Performance Analysis\n",
    "   - Compare metrics across implementations\n",
    "   - Identify strengths and weaknesses\n",
    "   - Note any patterns or trends\n",
    "\n",
    "2. Resource Usage\n",
    "   - Compare computational requirements\n",
    "   - Analyze response times\n",
    "   - Consider scaling implications\n",
    "\n",
    "3. Quality Assessment\n",
    "   - Evaluate answer quality\n",
    "   - Compare context relevance\n",
    "   - Assess faithfulness to sources\n",
    "\n",
    "4. Recommendations\n",
    "   - Suggest optimal use cases\n",
    "   - Identify areas for improvement\n",
    "   - Consider trade-offs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
