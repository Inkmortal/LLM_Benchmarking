{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Implementation Comparison Template\n",
    "\n",
    "This notebook provides a template for comparing different RAG implementations. It can be used to evaluate and compare any RAG techniques, including but not limited to:\n",
    "- Baseline RAG vs GraphRAG\n",
    "- Different vector stores\n",
    "- Different chunking strategies\n",
    "- Different embedding models\n",
    "- Different retrieval methods\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Document Processing\n",
    "- chunk_size: Number of words per chunk (default: 500)\n",
    "- chunk_overlap: Number of overlapping words between chunks (default: 50)\n",
    "- enable_chunking: Whether to split documents into chunks (default: True)\n",
    "\n",
    "### Vector Search\n",
    "- k: Number of context documents to retrieve (default: 3)\n",
    "- search_type: Type of vector search to use ('script' or 'knn', default: 'script')\n",
    "- similarity_threshold: Minimum similarity score to include (default: None)\n",
    "\n",
    "### OpenSearch\n",
    "- index_settings: Custom index settings for performance tuning\n",
    "- knn_params: Parameters for k-NN algorithm (e.g., ef_search)\n",
    "\n",
    "### API Settings\n",
    "- max_retries: Maximum number of retry attempts (default: 5)\n",
    "- min_delay: Minimum delay between retries in seconds (default: 1)\n",
    "- max_delay: Maximum delay between retries in seconds (default: 60)\n",
    "\n",
    "## Prerequisites\n",
    "- Run setup.ipynb first to configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.aws.opensearch_utils import OpenSearchManager\n",
    "from utils.metrics.rag_metrics import RAGMetricsEvaluator\n",
    "from utils.notebook_utils.dataset_utils import (\n",
    "    load_labeled_dataset,\n",
    "    examine_dataset_structure,\n",
    "    save_dataset_info\n",
    ")\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implementation Configuration\n",
    "IMPLEMENTATIONS = {\n",
    "    \"implementation_1\": {\n",
    "        \"name\": \"baseline_rag\",\n",
    "        \"notebook_path\": \"../../rag_implementations/baseline_rag/implementation.ipynb\",\n",
    "        \"config\": {\n",
    "            # Document processing\n",
    "            \"chunk_size\": 500,  # 500 words â‰ˆ 2000 chars\n",
    "            \"chunk_overlap\": 50,  # 50 words overlap\n",
    "            \"enable_chunking\": True,\n",
    "            \n",
    "            # Vector search\n",
    "            \"search_type\": \"script\",  # 'script' or 'knn'\n",
    "            \"similarity_threshold\": None,  # Minimum similarity score\n",
    "            \n",
    "            # OpenSearch settings\n",
    "            \"index_settings\": {\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0,\n",
    "                \"knn\": {\n",
    "                    \"algo_param\": {\n",
    "                        \"ef_search\": 512  # Higher = more accurate but slower\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"knn_params\": {\n",
    "                \"ef_construction\": 512,  # Higher = more accurate index\n",
    "                \"m\": 16  # Higher = more connections per node\n",
    "            },\n",
    "            \n",
    "            # API settings\n",
    "            \"max_retries\": 5,\n",
    "            \"min_delay\": 1.0,\n",
    "            \"max_delay\": 60.0\n",
    "        }\n",
    "    },\n",
    "    \"implementation_2\": {\n",
    "        \"name\": \"graph_rag\",\n",
    "        \"notebook_path\": \"../../rag_implementations/graph_rag/implementation.ipynb\",\n",
    "        \"config\": {\n",
    "            # Add GraphRAG specific configuration\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASETS = {\n",
    "    \"labeled\": {\n",
    "        \"covid19\": {\n",
    "            \"name\": \"OriginOfCovid19Dataset\",\n",
    "            \"dir\": \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "        }\n",
    "    },\n",
    "    \"unlabeled\": {\n",
    "        # Add unlabeled datasets here\n",
    "    }\n",
    "}\n",
    "\n",
    "# OpenSearch Configuration\n",
    "OPENSEARCH_CONFIG = {\n",
    "    \"domain_prefix\": \"rag-comparison\",\n",
    "    \"cleanup_resources\": True  # Default to cleaning up to avoid unexpected costs\n",
    "}\n",
    "\n",
    "print(\"Note: This notebook uses Amazon OpenSearch which incurs costs.\")\n",
    "print(\"CLEANUP_RESOURCES is enabled by default to delete resources after benchmarking.\")\n",
    "print(\"Set CLEANUP_RESOURCES = False if you want to preserve the OpenSearch domain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_implementation(impl_config: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Load a RAG implementation from notebook.\"\"\"\n",
    "    try:\n",
    "        module = notebook_to_module(impl_config[\"notebook_path\"])\n",
    "        implementation = module.RAGImplementation(**impl_config[\"config\"])\n",
    "        return implementation\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {impl_config['name']}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load implementations\n",
    "implementations = {}\n",
    "for impl_id, impl_config in IMPLEMENTATIONS.items():\n",
    "    implementations[impl_id] = load_implementation(impl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_opensearch(implementation_name: str) -> OpenSearchManager:\n",
    "    \"\"\"Set up OpenSearch for an implementation.\"\"\"\n",
    "    domain_name = f\"{OPENSEARCH_CONFIG['domain_prefix']}-{implementation_name}\"\n",
    "    \n",
    "    print(f\"Setting up OpenSearch for {implementation_name}...\")\n",
    "    manager = OpenSearchManager(\n",
    "        domain_name=domain_name,\n",
    "        cleanup_enabled=OPENSEARCH_CONFIG[\"cleanup_resources\"],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Set up domain and get endpoint\n",
    "    endpoint = manager.setup_domain()\n",
    "    os.environ['OPENSEARCH_HOST'] = endpoint\n",
    "    \n",
    "    return manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_evaluation(implementation: Any, dataset: Dataset, evaluator: RAGMetricsEvaluator):\n",
    "    \"\"\"Run evaluation for a single implementation on a dataset.\"\"\"\n",
    "    print(f\"Running evaluation...\")\n",
    "    total = len(dataset.examples)\n",
    "    \n",
    "    # Progress bar for overall evaluation\n",
    "    with tqdm_notebook(total=total, desc=\"Evaluating\") as pbar:\n",
    "        # Generate answers with progress tracking\n",
    "        questions = []\n",
    "        contexts = []\n",
    "        answers = []\n",
    "        references = []\n",
    "        \n",
    "        for i, example in enumerate(dataset.examples):\n",
    "            try:\n",
    "                result = implementation.query(example.query)\n",
    "                \n",
    "                questions.append(example.query)\n",
    "                contexts.append([doc['content'] for doc in result['context']])\n",
    "                answers.append(result['response'])\n",
    "                references.append(example.reference_answer)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': 'Success'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pbar.set_postfix({\n",
    "                    'Query': f\"{i+1}/{total}\",\n",
    "                    'Status': f'Error: {type(e).__name__}'\n",
    "                })\n",
    "                raise\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"reference\": references\n",
    "    }\n",
    "    eval_dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    # Evaluate results\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    try:\n",
    "        results = evaluator.evaluate_labeled(\n",
    "            queries=questions,\n",
    "            contexts=contexts,\n",
    "            generated_answers=answers,\n",
    "            reference_answers=references,\n",
    "            plot_results=True\n",
    "        )\n",
    "        \n",
    "        # Convert results to pandas DataFrame\n",
    "        df = results.to_pandas()\n",
    "        \n",
    "        # Return both raw results and DataFrame\n",
    "        return {\n",
    "            'raw_results': results,\n",
    "            'metrics_df': df.to_dict(),\n",
    "            'data': data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {type(e).__name__}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        print(\"\\nDataset contents:\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"\\n{key}:\")\n",
    "            print(f\"Type: {type(value)}\")\n",
    "            print(f\"Length: {len(value)}\")\n",
    "            print(f\"First item: {value[0][:100]}...\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize evaluator\n",
    "evaluator = RAGMetricsEvaluator()\n",
    "\n",
    "# Store results for each implementation\n",
    "all_results = {}\n",
    "\n",
    "# Run evaluation for each implementation\n",
    "for impl_id, implementation in implementations.items():\n",
    "    impl_name = IMPLEMENTATIONS[impl_id][\"name\"]\n",
    "    print(f\"\\nEvaluating {impl_name}...\")\n",
    "    \n",
    "    # Set up OpenSearch\n",
    "    manager = setup_opensearch(impl_name)\n",
    "    \n",
    "    # Store results for this implementation\n",
    "    impl_results = {}\n",
    "    \n",
    "    # Evaluate on each dataset\n",
    "    for dataset_type, datasets in DATASETS.items():\n",
    "        for dataset_id, dataset_config in datasets.items():\n",
    "            print(f\"\\nEvaluating on {dataset_config['name']}...\")\n",
    "            \n",
    "            # Load dataset\n",
    "            dataset_dir = project_root / dataset_config[\"dir\"]\n",
    "            dataset, documents = load_labeled_dataset(dataset_dir)\n",
    "            \n",
    "            # Run evaluation\n",
    "            results = run_evaluation(implementation, dataset, evaluator)\n",
    "            impl_results[dataset_id] = results\n",
    "    \n",
    "    # Store results\n",
    "    all_results[impl_id] = impl_results\n",
    "    \n",
    "    # Cleanup OpenSearch\n",
    "    manager.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_results(results: Dict[str, Any]):\n",
    "    \"\"\"Save evaluation results.\"\"\"\n",
    "    results_dir = project_root / \"evaluation_pipelines/rag_evaluations/results\"\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare results data\n",
    "    results_data = {\n",
    "        'implementations': IMPLEMENTATIONS,\n",
    "        'datasets': DATASETS,\n",
    "        'results': {}\n",
    "    }\n",
    "    \n",
    "    # Format results for each implementation\n",
    "    for impl_id, impl_results in results.items():\n",
    "        impl_name = IMPLEMENTATIONS[impl_id][\"name\"]\n",
    "        results_data['results'][impl_name] = {}\n",
    "        \n",
    "        for dataset_id, dataset_results in impl_results.items():\n",
    "            results_data['results'][impl_name][dataset_id] = {\n",
    "                'metrics': dataset_results['metrics_df'],\n",
    "                'evaluation_data': {\n",
    "                    'questions': dataset_results['data']['question'],\n",
    "                    'answers': dataset_results['data']['answer'],\n",
    "                    'references': dataset_results['data']['reference']\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = results_dir / f'rag_comparison_results_{timestamp}.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Save all results\n",
    "save_results(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def test_example_queries(implementation: Any):\n",
    "    \"\"\"Test implementation with example queries.\"\"\"\n",
    "    example_queries = [\n",
    "        \"What is the main focus of the article?\",\n",
    "        \"What evidence supports the main argument?\",\n",
    "        \"What are the key implications discussed?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing example queries...\\n\")\n",
    "    for query in example_queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        result = implementation.query(query)\n",
    "        print(f\"Response: {result['response']}\\n\")\n",
    "\n",
    "# Test each implementation\n",
    "for impl_id, implementation in implementations.items():\n",
    "    impl_name = IMPLEMENTATIONS[impl_id][\"name\"]\n",
    "    print(f\"\\nTesting {impl_name}...\")\n",
    "    test_example_queries(implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "Use this section to analyze the results and document insights about the comparison:\n",
    "\n",
    "1. Performance Analysis\n",
    "   - Compare metrics across implementations\n",
    "   - Identify strengths and weaknesses\n",
    "   - Note any patterns or trends\n",
    "\n",
    "2. Resource Usage\n",
    "   - Compare computational requirements\n",
    "   - Analyze response times\n",
    "   - Consider scaling implications\n",
    "\n",
    "3. Quality Assessment\n",
    "   - Evaluate answer quality\n",
    "   - Compare context relevance\n",
    "   - Assess faithfulness to sources\n",
    "\n",
    "4. Recommendations\n",
    "   - Suggest optimal use cases\n",
    "   - Identify areas for improvement\n",
    "   - Consider trade-offs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
