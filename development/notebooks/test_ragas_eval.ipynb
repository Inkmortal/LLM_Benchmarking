{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RAGAs Evaluation\n",
    "\n",
    "This notebook provides a minimal test setup for RAGAs evaluation.\n",
    "\n",
    "## Features\n",
    "- Small dataset subset for quick testing\n",
    "- Cache clearing for rapid iterations\n",
    "- Detailed error reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    ContextRecall,\n",
    "    ContextPrecision,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_aws import ChatBedrockConverse, BedrockEmbeddings\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"../..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import utilities\n",
    "from utils.aws.opensearch_utils import OpenSearchManager\n",
    "from utils.notebook_utils.dataset_utils import load_labeled_dataset\n",
    "from utils.notebook_utils.importable import notebook_to_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear module cache to pick up changes\n",
    "def clear_module_cache():\n",
    "    \"\"\"Clear imported module cache to pick up changes\"\"\"\n",
    "    modules_to_clear = [\n",
    "        m for m in sys.modules\n",
    "        if m.startswith('utils.') or \n",
    "           m.startswith('rag_implementations.')\n",
    "    ]\n",
    "    for module in modules_to_clear:\n",
    "        del sys.modules[module]\n",
    "    print(f\"Cleared {len(modules_to_clear)} modules from cache\")\n",
    "\n",
    "clear_module_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenSearch Setup\n",
    "OPENSEARCH_DOMAIN = \"baseline-rag-benchmark-store\"\n",
    "INDEX_NAME = \"originofcovid19dataset-benchmark\"  # Fixed index name\n",
    "\n",
    "print(\"Setting up OpenSearch...\")\n",
    "manager = OpenSearchManager(\n",
    "    domain_name=OPENSEARCH_DOMAIN,\n",
    "    cleanup_enabled=False,  # Don't clean up, we want to keep the index\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Get endpoint and set environment variable\n",
    "endpoint = manager.setup_domain()\n",
    "os.environ['OPENSEARCH_HOST'] = endpoint\n",
    "print(f\"OpenSearch endpoint: {endpoint}\")\n",
    "\n",
    "# Create OpenSearch client\n",
    "region = boto3.Session().region_name\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    'es',\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "opensearch = OpenSearch(\n",
    "    hosts=[{'host': endpoint, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Check index contents\n",
    "print(\"\\nChecking index...\")\n",
    "try:\n",
    "    # Search for all documents\n",
    "    response = opensearch.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"size\": 1  # Just get one to check if index has content\n",
    "        }\n",
    "    )\n",
    "    total_docs = response['hits']['total']['value']\n",
    "    print(f\"Found {total_docs} documents in index {INDEX_NAME}\")\n",
    "    \n",
    "    if total_docs > 0:\n",
    "        # Show sample document\n",
    "        sample_doc = response['hits']['hits'][0]['_source']\n",
    "        print(\"\\nSample document:\")\n",
    "        print(f\"Content length: {len(sample_doc['content'])} chars\")\n",
    "        print(f\"Content preview: {sample_doc['content'][:200]}...\")\n",
    "        print(f\"Metadata: {sample_doc.get('metadata', {})}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error checking index: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small subset of data\n",
    "DATASET_DIR = project_root / \"datasets/rag_evaluation/labeled/covid19_origin\"\n",
    "NUM_TEST_SAMPLES = 3  # Small subset for testing\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset, documents = load_labeled_dataset(DATASET_DIR)\n",
    "test_examples = dataset.examples[:NUM_TEST_SAMPLES]\n",
    "print(f\"Using {len(test_examples)} test examples\")\n",
    "\n",
    "# Show example structure\n",
    "example = test_examples[0]\n",
    "print(\"\\nExample structure:\")\n",
    "print(f\"Query: {example.query}\")\n",
    "print(f\"Reference answer: {example.reference_answer}\")\n",
    "print(f\"Number of reference contexts: {len(example.reference_contexts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RAG implementation\n",
    "implementation_path = str(project_root / 'rag_implementations/baseline_rag/implementation.ipynb')\n",
    "BaselineRAG = notebook_to_module(implementation_path).BaselineRAG\n",
    "\n",
    "# Initialize RAG with same index name\n",
    "rag = BaselineRAG(\n",
    "    index_name=INDEX_NAME,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    enable_chunking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search directly\n",
    "print(\"Testing semantic search...\")\n",
    "example = test_examples[0]\n",
    "print(f\"\\nQuery: {example.query}\")\n",
    "\n",
    "# Get relevant documents\n",
    "docs = rag.semantic_search(example.query, k=3)\n",
    "print(f\"\\nRetrieved {len(docs)} documents\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Content length: {len(doc['content'])} chars\")\n",
    "    print(f\"Content preview: {doc['content'][:200]}...\")\n",
    "    print(f\"Metadata: {doc.get('metadata', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers and collect contexts\n",
    "print(\"Generating answers...\")\n",
    "questions = []\n",
    "answers = []\n",
    "contexts = []\n",
    "references = []\n",
    "\n",
    "for example in test_examples:\n",
    "    print(f\"\\nQuery: {example.query}\")\n",
    "    result = rag.query(example.query)\n",
    "    \n",
    "    questions.append(example.query)\n",
    "    answers.append(result['response'])\n",
    "    contexts.append([doc['content'] for doc in result['context']])\n",
    "    references.append(example.reference_answer)  # Single string reference\n",
    "    \n",
    "    print(f\"Retrieved {len(result['context'])} context documents\")\n",
    "    print(f\"Answer: {result['response'][:100]}...\")\n",
    "    print(f\"Reference: {example.reference_answer[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAGAs dataset\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"reference\": references  # Single string references\n",
    "}\n",
    "\n",
    "# Convert to dataset\n",
    "eval_dataset = Dataset.from_dict(data)\n",
    "print(\"Dataset structure:\")\n",
    "print(eval_dataset)\n",
    "\n",
    "# Show first example\n",
    "print(\"\\nFirst example:\")\n",
    "example = eval_dataset[0]\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Answer: {example['answer'][:100]}...\")\n",
    "print(f\"Number of contexts: {len(example['contexts'])}\")\n",
    "print(f\"Reference: {example['reference'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS config\n",
    "config = {\n",
    "    \"region_name\": \"us-west-2\",\n",
    "    \"llm\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"embeddings\": \"cohere.embed-english-v3\",\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "# Initialize evaluator models\n",
    "evaluator_llm = LangchainLLMWrapper(ChatBedrockConverse(\n",
    "    region_name=config[\"region_name\"],\n",
    "    base_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "    model=config[\"llm\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "))\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(BedrockEmbeddings(\n",
    "    region_name=config[\"region_name\"],\n",
    "    model_id=config[\"embeddings\"],\n",
    "))\n",
    "\n",
    "# Initialize metrics with wrapped models\n",
    "metrics = [\n",
    "    # Context retrieval metrics\n",
    "    ContextPrecision(llm=evaluator_llm),\n",
    "    ContextRecall(llm=evaluator_llm),\n",
    "    ContextEntityRecall(llm=evaluator_llm),\n",
    "    NoiseSensitivity(llm=evaluator_llm),\n",
    "    \n",
    "    # Answer generation metrics\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running evaluation...\")\n",
    "try:\n",
    "    results = evaluate(\n",
    "        dataset=eval_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    # Convert to pandas for better display\n",
    "    df = results.to_pandas()\n",
    "    print(\"\\nResults:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Group metrics by type\n",
    "    print(\"\\nContext Retrieval Metrics:\")\n",
    "    retrieval_metrics = ['context_precision', 'context_recall', 'context_entity_recall', 'noise_sensitivity_relevant']\n",
    "    retrieval_scores = df[retrieval_metrics].mean()\n",
    "    print(retrieval_scores)\n",
    "    \n",
    "    print(\"\\nAnswer Generation Metrics:\")\n",
    "    generation_metrics = ['faithfulness', 'answer_relevancy']\n",
    "    generation_scores = df[generation_metrics].mean()\n",
    "    print(generation_scores)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot retrieval metrics\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=retrieval_scores.index, y=retrieval_scores.values)\n",
    "    plt.title('Context Retrieval Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot generation metrics\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=generation_scores.index, y=generation_scores.values)\n",
    "    plt.title('Answer Generation Metrics')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nAnalysis:\")\n",
    "    print(\"Context Retrieval:\")\n",
    "    print(f\"- Precision: {retrieval_scores['context_precision']:.2f} - How many retrieved documents are relevant\")\n",
    "    print(f\"- Recall: {retrieval_scores['context_recall']:.2f} - How many relevant documents were retrieved\")\n",
    "    print(f\"- Entity Recall: {retrieval_scores['context_entity_recall']:.2f} - How well important entities are preserved\")\n",
    "    print(f\"- Noise Sensitivity: {retrieval_scores['noise_sensitivity_relevant']:.2f} - Robustness against irrelevant data\")\n",
    "    \n",
    "    print(\"\\nAnswer Generation:\")\n",
    "    print(f\"- Faithfulness: {generation_scores['faithfulness']:.2f} - How factual the answers are\")\n",
    "    print(f\"- Relevancy: {generation_scores['answer_relevancy']:.2f} - How relevant the answers are to questions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {type(e).__name__}\")\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    print(\"\\nDataset contents:\")\n",
    "    for key, value in data.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"Type: {type(value)}\")\n",
    "        print(f\"Length: {len(value)}\")\n",
    "        print(f\"First item: {value[0][:100]}...\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
